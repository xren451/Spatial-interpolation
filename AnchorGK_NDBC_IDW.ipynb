{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T05:55:20.633152Z",
     "start_time": "2024-06-23T05:55:20.626192Z"
    }
   },
   "source": [
    "###############\n",
    "##Version 0:IDW on NDBC \n",
    "###############"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T05:55:24.829740Z",
     "start_time": "2024-06-23T05:55:24.003912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import six.moves.cPickle as pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Correct file path\n",
    "# file_path = '/home/xren451/rxb/phd/Spatial_interpolation/XBSPA/ModIGNNK/data/NDBC/all.npy'\n",
    "file_path = 'data/NDBC/all.npy'\n",
    "station_value = np.load(file_path)\n",
    "station_value=station_value.transpose(2,0,1)\n",
    "lat_file_path = 'data/NDBC/Station_info_edit.csv'\n",
    "station_info=np.array(pd.read_csv(lat_file_path,header=None))\n",
    "station_value = station_value[:, :, 5:13]\n",
    "station_value=torch.tensor(station_value)"
   ],
   "id": "8707d0e087936f8b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T06:14:38.160837Z",
     "start_time": "2024-06-23T06:14:38.132560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lat_file_path = 'data/NDBC/Station_info_edit.csv'\n",
    "station_info=pd.read_csv(lat_file_path,header=None)\n",
    "station_info.head()"
   ],
   "id": "ca2c5153f3709972",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       0       1       2\n",
       "0  41001  34.703  72.242\n",
       "1  41008  31.400  80.866\n",
       "2  41012  30.042  80.534\n",
       "3  41013  33.441  77.764\n",
       "4  41024  33.837  78.477"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41001</td>\n",
       "      <td>34.703</td>\n",
       "      <td>72.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41008</td>\n",
       "      <td>31.400</td>\n",
       "      <td>80.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41012</td>\n",
       "      <td>30.042</td>\n",
       "      <td>80.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41013</td>\n",
       "      <td>33.441</td>\n",
       "      <td>77.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41024</td>\n",
       "      <td>33.837</td>\n",
       "      <td>78.477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T06:24:54.124151Z",
     "start_time": "2024-06-23T06:24:53.646234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import random\n",
    "\n",
    "def idw_interpolation(target_coord, known_coords, known_values, power=2):\n",
    "    distances = distance_matrix([target_coord], known_coords)[0]\n",
    "    # Handle case where distance is zero (i.e., target_coord is exactly at a known_coord)\n",
    "    if np.any(distances == 0):\n",
    "        return known_values[np.argmin(distances)]\n",
    "    weights = 1 / (distances ** power)\n",
    "    weights /= weights.sum()  # Normalize weights\n",
    "    interpolated_value = np.dot(weights, known_values)\n",
    "    return interpolated_value\n",
    "\n",
    "def perform_idw_for_feature(station_info, station_values, feature_index, target_coords):\n",
    "    feature_values = station_values[:, :, feature_index]  # Extract specific feature values\n",
    "    interpolated_values = []\n",
    "    \n",
    "    for t in range(feature_values.shape[1]):  # Iterate over timesteps\n",
    "        known_indices = ~torch.isnan(feature_values[:, t])  # Identify stations with known values\n",
    "        if known_indices.sum() == 0:\n",
    "            interpolated_values.append(np.nan)  # If no known values, return NaN\n",
    "            continue\n",
    "        \n",
    "        known_coords = station_info[known_indices.numpy(), 1:3]  # Coordinates of known stations (2D)\n",
    "        known_values = feature_values[known_indices, t].numpy()  # Known feature values\n",
    "        \n",
    "        interpolated_value = idw_interpolation(target_coords, known_coords, known_values)\n",
    "        interpolated_values.append(interpolated_value)\n",
    "    \n",
    "    return np.array(interpolated_values)\n",
    "\n",
    "def compute_mse(true_values, predicted_values):\n",
    "    return np.nanmean((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Example usage\n",
    "lat_file_path = 'data/NDBC/Station_info_edit.csv'\n",
    "station_info = pd.read_csv(lat_file_path, header=None).values  # Convert to NumPy array directly\n",
    "station_values_tensor = torch.tensor(station_value)  # Assuming station_values is your tensor\n",
    "\n",
    "feature_index = 0   # Specify the feature index you are interested in\n",
    "\n",
    "# Randomly select a target station\n",
    "target_index = random.choice(range(station_info.shape[0]))\n",
    "target_coords = station_info[target_index, 1:3]  # Assuming the coordinates are in columns 1 and 2\n",
    "\n",
    "# Get the true values for the selected station\n",
    "true_values = station_values_tensor[target_index, :, feature_index].numpy()\n",
    "\n",
    "# Perform IDW interpolation\n",
    "interpolated_results = perform_idw_for_feature(station_info, station_values_tensor, feature_index, target_coords)\n",
    "\n",
    "# Compute the MSE\n",
    "mse = compute_mse(true_values, interpolated_results)\n",
    "print(f\"Target station index: {target_index}\")\n",
    "print(f\"True values: {true_values}\")\n",
    "print(f\"Interpolated values: {interpolated_results}\")\n",
    "print(f\"MSE: {mse}\")\n"
   ],
   "id": "656c5a61e1538687",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1020502/2591001307.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  station_values_tensor = torch.tensor(station_value)  # Assuming station_values is your tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target station index: 44\n",
      "True values: [205.53244 205.53244 205.53244 ... 287.      290.      282.     ]\n",
      "Interpolated values: [205.53244 205.53244 205.53244 ... 287.      290.      282.     ]\n",
      "MSE: 0.0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GCNConv\n",
    "\n",
    "# Load datasets\n",
    "station_value_path = 'data/NDBC/all.npy'\n",
    "station_info_path = 'data/NDBC/Station_info_edit.csv'\n",
    "\n",
    "station_value = np.load(station_value_path)\n",
    "station_value = station_value.transpose(2, 0, 1)[:, :, 5:13]  # Select relevant features\n",
    "station_info = pd.read_csv(station_info_path, header=None).values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "station_value = station_value.reshape(-1, station_value.shape[-1])\n",
    "station_value = scaler.fit_transform(station_value)\n",
    "station_value = station_value.reshape(103, 8784, 8)\n",
    "\n",
    "# Utility functions\n",
    "def idw_interpolation(station_coords, temperatures, target_point, power=2):\n",
    "    distances = np.linalg.norm(station_coords - target_point, axis=1)\n",
    "    weights = 1 / (distances ** power)\n",
    "    weights /= weights.sum()\n",
    "    interpolated_value = np.dot(weights, temperatures)\n",
    "    return interpolated_value\n",
    "\n",
    "def calculate_inverse_distance_adj(station_coords):\n",
    "    num_nodes = station_coords.shape[0]\n",
    "    adj = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                distance = np.linalg.norm(station_coords[i] - station_coords[j])\n",
    "                adj[i, j] = np.exp(-distance)\n",
    "    np.fill_diagonal(adj, 1)  # Add self-loops\n",
    "    return adj\n",
    "\n",
    "def GCN_model(input_shape, adj_shape):\n",
    "    inputs_feat = Input(shape=(input_shape,))\n",
    "    inputs_adj = Input(shape=(adj_shape[0], adj_shape[1]), sparse=True)\n",
    "    gcn_output = GCNConv(16, activation='relu')([inputs_feat, inputs_adj])\n",
    "    model = Model(inputs=[inputs_feat, inputs_adj], outputs=gcn_output)\n",
    "    return model\n",
    "\n",
    "# Convert station_info to a dictionary for easy lookup\n",
    "station_coords_dict = {row[0]: (float(row[1]), float(row[2])) for row in station_info}\n",
    "station_coords = np.array(list(station_coords_dict.values()))  # Convert to numpy array for vectorized operations\n",
    "\n",
    "# Prepare data for GCN\n",
    "adj_matrix = calculate_inverse_distance_adj(station_info[:, 1:3].astype(float))\n",
    "adj_matrix_sparse = tf.sparse.from_dense(adj_matrix)\n",
    "x = station_value[:, :, target_feature].transpose(1, 0)  # (time_steps, num_stations, 1)\n",
    "\n",
    "# Ensure the input to the GCN is 2D\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "# Define and train the GCN model\n",
    "gcn_model = GCN_model(x.shape[1], adj_matrix.shape)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    for batch in range(num_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_indices = range(batch * batch_size, (batch + 1) * batch_size)\n",
    "            gcn_outputs = []\n",
    "            for t in batch_indices:\n",
    "                gcn_output = gcn_model([x[t], adj_matrix_sparse])\n",
    "                gcn_outputs.append(gcn_output)\n",
    "\n",
    "            H_gcn = tf.stack(gcn_outputs, axis=0)\n",
    "            H_gcn = tf.reshape(H_gcn, (batch_size, station_info.shape[0], -1))  # (batch_size, num_stations, 16)\n",
    "\n",
    "            # MoE aggregation\n",
    "            H_pos_all = np.random.rand(station_info.shape[0], 16)  # Dummy positional encoding for stations\n",
    "            H_pos_one = np.random.rand(16)  # Dummy positional encoding for target station\n",
    "            estimated_values = tf.einsum('bnd,nd->bn', H_gcn, H_pos_all)  # (batch_size, num_stations)\n",
    "            estimated_values = tf.einsum('bn,d->b', estimated_values, H_pos_one)  # (batch_size,)\n",
    "\n",
    "            batch_true_values = idw_estimate[batch_indices]\n",
    "            loss = loss_fn(batch_true_values, estimated_values)\n",
    "\n",
    "        grads = tape.gradient(loss, gcn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, gcn_model.trainable_variables))\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "    epoch_loss /= num_batches\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}')\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict final values\n",
    "gcn_outputs = []\n",
    "for t in range(x.shape[0]):\n",
    "    gcn_output = gcn_model([x[t], adj_matrix_sparse])\n",
    "    gcn_outputs.append(gcn_output)\n",
    "\n",
    "H_gcn = tf.stack(gcn_outputs, axis=0)\n",
    "H_gcn = tf.reshape(H_gcn, (x.shape[0], station_info.shape[0], -1))  # (time_steps, num_stations, 16)\n",
    "\n",
    "# MoE aggregation for predictions\n",
    "H_pos_all = np.random.rand(station_info.shape[0], 16)  # Dummy positional encoding for stations\n",
    "H_pos_one = np.random.rand(16)  # Dummy positional encoding for target station\n",
    "estimated_values = tf.einsum('tnd,nd->tn', H_gcn, H_pos_all)  # (time_steps, num_stations)\n",
    "estimated_values = tf.einsum('tn,d->t', estimated_values, H_pos_one)  # (time_steps,)\n",
    "\n",
    "# Convert Tensor to NumPy array before reshaping\n",
    "estimated_values = estimated_values.numpy()\n",
    "\n",
    "# Denormalize the predictions\n",
    "estimated_values = scaler.inverse_transform(estimated_values.reshape(-1, 1)).flatten()\n",
    "idw_estimate = scaler.inverse_transform(idw_estimate.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Compute MAE and RMSE\n",
    "mae = mean_absolute_error(idw_estimate, estimated_values)\n",
    "rmse = mean_squared_error(idw_estimate, estimated_values, squared=False)\n",
    "print(f'MAE: {mae}, RMSE: {rmse}, Training Time: {training_time} seconds')\n"
   ],
   "id": "db48c4373a67d5ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
