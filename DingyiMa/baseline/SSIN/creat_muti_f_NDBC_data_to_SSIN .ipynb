{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geographiclib.geodesic import Geodesic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(random.randint(0, 1))\n",
    "    return array\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "random_array = generate_random_array(10)\n",
    "print(random_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=np.load('data/all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 103)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=14\n",
    "train_data=X_raw[:,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "missing_columns = np.where(columns_with_missing_data)[0]\n",
    "result = np.delete(train_data, missing_columns, axis=1)\n",
    "result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  15,  28,  34,  45,  47,  55,  60,  82,  83,  84,  85,  86,\n",
       "        87,  88,  89,  90,  91,  92,  93,  94,  96,  98,  99, 100, 102],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 77)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result=train_data\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = result.mean(axis=0)\n",
    "# std = result.std(axis=0)\n",
    "# result = (result - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70]\n",
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]\n",
    "test_list=[52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76]\n",
    "# test_list=[71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=result[:,train_list]\n",
    "test_data=result[:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.transpose(train_data, (0, 2, 1))\n",
    "# test_data = np.transpose(test_data, (0, 2, 1))\n",
    "# train_data = np.reshape(train_data, (8784, 63,1))\n",
    "# test_data = np.reshape(test_data, (8784, 40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 1)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 1)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 103)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = np.delete(train_data, missing_columns, axis=1)\n",
    "# X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=5\n",
    "# train_data=X_raw[:,j,:]\n",
    "train_fusion_data=np.delete(X_raw, missing_columns, axis=2)\n",
    "# train_fusion_data[train_fusion_data == 0] = 0.00001\n",
    "# train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)\n",
    "# train_fusion_data=X_raw\n",
    "# train_data = np.nan_to_num(train_data, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fusion_data[train_fusion_data == 0] = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 77)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 14, 77)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_remove = [11, 15, 16 ,17]\n",
    "train_fusion_data = np.delete(train_fusion_data, columns_to_remove, axis=1)\n",
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 17568\n",
      "Number of zeros in the 6th feature: 254736\n",
      "Number of zeros in the 6th feature: 254736\n",
      "Number of zeros in the 6th feature: 509472\n",
      "Number of zeros in the 6th feature: 26352\n",
      "Number of zeros in the 6th feature: 17568\n",
      "Number of zeros in the 6th feature: 755424\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5,train_fusion_data.shape[1]):\n",
    "#     nan_count = np.sum(np.isnan(train_fusion_data[:,i,:]))\n",
    "#     print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_index in range(5, train_fusion_data.shape[1]):\n",
    "    feature_column = train_fusion_data[:, feature_index, :]\n",
    "    min=np.nanmin(feature_column)\n",
    "    max=np.nanmax(feature_column)\n",
    "    norm_data=(feature_column - min) / (max - min)\n",
    "    train_fusion_data[:, feature_index, :]=norm_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data[train_fusion_data == 0] = 0.00001\n",
    "train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5,train_fusion_data.shape[1]):\n",
    "#     nan_count = np.sum(np.isnan(train_fusion_data[:,i,:]))\n",
    "#     print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 254736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # train_fusion_data[:,5,:]\n",
    "# nan_count = np.sum(np.isnan(norm_data))\n",
    "# print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "# missing_columns = np.where(columns_with_missing_data)[0]\n",
    "# result = np.delete(train_data, missing_columns, axis=2)\n",
    "# # result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_arr = np.unique(missing_columns)\n",
    "# unique_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.05000697e-01, 2.17916734e-01, 2.17916734e-01, ...,\n",
       "         2.17916734e-01, 2.19241456e-01, 2.18910276e-01],\n",
       "        [8.24267782e-01, 7.85216179e-01, 8.53556485e-01, ...,\n",
       "         8.84239888e-01, 8.67503487e-01, 8.82845188e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         6.87315634e-01, 6.72566372e-01, 7.22713864e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.06656599e-01, 2.19241456e-01, 2.18910276e-01, ...,\n",
       "         2.19241456e-01, 2.20234997e-01, 2.19903817e-01],\n",
       "        [8.20083682e-01, 7.88005579e-01, 8.56345886e-01, ...,\n",
       "         8.84239888e-01, 8.66108787e-01, 8.81450488e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         6.84365782e-01, 6.72566372e-01, 7.19764012e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.08974862e-01, 2.19903817e-01, 2.18910276e-01, ...,\n",
       "         2.21228539e-01, 2.21890900e-01, 2.21559719e-01],\n",
       "        [8.15899582e-01, 7.86610879e-01, 8.59135286e-01, ...,\n",
       "         8.84239888e-01, 8.67503487e-01, 8.82845188e-01],\n",
       "        [6.43067847e-01, 4.89675516e-01, 6.87315634e-01, ...,\n",
       "         6.84365782e-01, 6.72566372e-01, 7.19764012e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.23546802e-01, 2.29176870e-01, 2.25865065e-01, ...,\n",
       "         2.21228539e-01, 2.19572637e-01, 2.19241456e-01],\n",
       "        [7.61506276e-01, 7.28033473e-01, 8.29846583e-01, ...,\n",
       "         8.52161785e-01, 8.49372385e-01, 8.60529986e-01],\n",
       "        [6.22418879e-01, 4.18879056e-01, 6.81415929e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.24871524e-01, 2.28514509e-01, 2.26527426e-01, ...,\n",
       "         2.20897358e-01, 2.20234997e-01, 2.20566178e-01],\n",
       "        [7.67085077e-01, 7.25244073e-01, 8.32635983e-01, ...,\n",
       "         8.52161785e-01, 8.49372385e-01, 8.59135286e-01],\n",
       "        [6.22418879e-01, 4.18879056e-01, 6.81415929e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]],\n",
       "\n",
       "       [[2.01100000e+03, 2.01100000e+03, 2.01100000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.02020073e-01, 2.15598471e-01, 2.16592013e-01, ...,\n",
       "         2.19241456e-01, 2.18910276e-01, 2.18579095e-01],\n",
       "        [8.27057183e-01, 7.88005579e-01, 8.50767085e-01, ...,\n",
       "         8.53556485e-01, 8.50767085e-01, 8.57740586e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]]])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=train_fusion_data\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 14, 77)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data shape: (8784, 14, 93)\n"
     ]
    }
   ],
   "source": [
    "# normalized_data = np.zeros_like(result)\n",
    "# for feature_index in range(result.shape[1]):  # 遍历每个特征\n",
    "#     feature_values = result[:, feature_index, :]  # 当前特征的所有数据点\n",
    "#     non_zero_mask = feature_values != 0  # 非零值掩码\n",
    "#     non_zero_values = feature_values[non_zero_mask]  # 提取非零值\n",
    "\n",
    "#     # 计算非零值的均值和标准差\n",
    "#     mean = np.mean(non_zero_values)\n",
    "#     std = np.std(non_zero_values)\n",
    "\n",
    "#     # 只在非零值上应用 Z-score 标准化\n",
    "#     if std > 0:\n",
    "#         normalized_values = (non_zero_values - mean) / std\n",
    "#         feature_values[non_zero_mask] = normalized_values  # 将计算结果赋值回相应位置\n",
    "#     else:\n",
    "#         # 如果所有非零值都相等（std=0），可以选择保留原值或其他处理\n",
    "#         feature_values[non_zero_mask] = non_zero_values  # 这里选择保留原值\n",
    "\n",
    "#     normalized_data[:, feature_index, :] = feature_values  # 更新标准化数据\n",
    "# print(\"Normalized data shape:\", normalized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70]\n",
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]\n",
    "test_list=[52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76]\n",
    "# test_list=[71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=result[:,:,train_list]\n",
    "test_fusion_data=result[:,:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data = np.transpose(train_fusion_data, (0, 2, 1))\n",
    "test_fusion_data = np.transpose(test_fusion_data, (0, 2, 1))\n",
    "# train_fusion_data = np.reshape(train_fusion_data, (8784, 63,18))\n",
    "# test_fusion_data = np.reshape(test_fusion_data, (8784, 40,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 14)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 14)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.reshape(train_data, (8784, 41,18))\n",
    "# test_data = np.reshape(test_data, (8784, 62,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_list=[0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=X_raw[:,timestap_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap.astype(np.int32)\n",
    "timestap=timestap.astype(np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timestap)):\n",
    "    for m in range(1,4):\n",
    "        timestap[i,m,1]=timestap[i,m,1].zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap[:,:,1]\n",
    "timestap_result=[]\n",
    "for i in range(len(timestap)):\n",
    "    x=timestap[i,0]+timestap[i,1]+timestap[i,2]+'-'+timestap[i,3]\n",
    "    timestap_result.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_result=np.array(timestap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784,)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestap_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invid_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(False)\n",
    "    return np.array(array)\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "train_invid_mask=[]\n",
    "test_invid_mask=[]\n",
    "for i in range(len(timestap)):\n",
    "    random_array = generate1_random_array(train_data.shape[1])\n",
    "    random_array1 = generate1_random_array(test_data.shape[1])\n",
    "    train_invid_mask.append(random_array)\n",
    "    test_invid_mask.append(random_array1)\n",
    "train_invid_mask=np.array(train_invid_mask)\n",
    "test_invid_mask=np.array(test_invid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_invid_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "            array.append(random.choice([False,True]))\n",
    "    return np.array(array)\n",
    "\n",
    "# 生成长度为10的随机数组\n",
    "test_mask=[]\n",
    "random_array=generate_random_array(test_data.shape[1])\n",
    "for i in range(len(timestap)):\n",
    "    test_mask.append(random_array)\n",
    "test_mask=np.array(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 地理位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建r_pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(\"D:\\Spatial_interpolation\\SSIN\\data\\Station_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.drop([  4,  15,  28,  34,  45,  47,  55,  60,  82,  83,  84,  85,  86,\n",
    "        87,  88,  89,  90,  91,  92,  93,  94,  96,  98,  99, 100, 102],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=stations.iloc[0:52]\n",
    "df_test=stations.iloc[52:77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_angle_mat(df, out_path):\n",
    "    lons, lats = df[\"lon\"].values, df[\"lat\"].values\n",
    "    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n",
    "\n",
    "    for i in range(len(lons)):\n",
    "        for j in range(len(lons)):\n",
    "            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n",
    "            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n",
    "            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n",
    "\n",
    "    print(dist_angle_mat.shape)\n",
    "    # print(dist_angle_mat)\n",
    "    np.save(out_path, dist_angle_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 52, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_train.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_train, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 25, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}/BW_132_data\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_test.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_test, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_from_train_data(train_info_df, r_pos_mat):\n",
    "    # info_df = pd.read_csv(info_path)\n",
    "    # ori_r_pos_mat = np.load(relative_pos_mat_path)\n",
    "\n",
    "    # is_test = info_df[\"is_test\"].values\n",
    "    # train_mask = np.where(is_test == 0, True, False)\n",
    "    # train_info_df = info_df.loc[train_mask, :]\n",
    "\n",
    "    lat_mean, lat_std, lat_max, lat_min = train_info_df[\"lat\"].mean(), train_info_df[\"lat\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lat\"].max(), train_info_df[\"lat\"].min()\n",
    "    lon_mean, lon_std, lon_max, lon_min = train_info_df[\"lon\"].mean(), train_info_df[\"lon\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lon\"].max(), train_info_df[\"lon\"].min()\n",
    "\n",
    "    # indexes = np.where(train_mask)[0]\n",
    "    # idx_i, idx_j = np.ix_(indexes, indexes)\n",
    "    # r_pos_mat = ori_r_pos_mat[idx_i, idx_j, :]\n",
    "\n",
    "    r_dist_mat = r_pos_mat[:, :, 0]\n",
    "    r_angle_mat = r_pos_mat[:, :, 1]\n",
    "\n",
    "    r_dist_mean, r_dist_std, r_dist_max, r_dist_min = np.mean(r_dist_mat), np.std(r_dist_mat), \\\n",
    "                                                      np.max(r_dist_mat), np.min(r_dist_mat),\n",
    "    r_angle_mean, r_angle_std, r_angle_max, r_angle_min = np.mean(r_angle_mat), np.std(r_angle_mat), \\\n",
    "                                                          np.max(r_angle_mat), np.min(r_angle_mat)\n",
    "\n",
    "    stat_dict = {}\n",
    "    stat_dict[\"lat_mean\"], stat_dict[\"lat_std\"], stat_dict[\"lat_max\"], stat_dict[\"lat_min\"] = \\\n",
    "        lat_mean, lat_std, lat_max, lat_min\n",
    "    stat_dict[\"lon_mean\"], stat_dict[\"lon_std\"], stat_dict[\"lon_max\"], stat_dict[\"lon_min\"] = \\\n",
    "        lon_mean, lon_std, lon_max, lon_min\n",
    "\n",
    "    stat_dict[\"r_dist_mean\"], stat_dict[\"r_dist_std\"], stat_dict[\"r_dist_max\"], stat_dict[\"r_dist_min\"] = \\\n",
    "        r_dist_mean, r_dist_std, r_dist_max, r_dist_min\n",
    "    stat_dict[\"r_angle_mean\"], stat_dict[\"r_angle_std\"], stat_dict[\"r_angle_max\"], stat_dict[\"r_angle_min\"] = \\\n",
    "        r_angle_mean, r_angle_std, r_angle_max, r_angle_min\n",
    "\n",
    "    print(\"Calculates the statistics of training data. Done!\")\n",
    "\n",
    "    # with open(\"./data/hk_data_stats.pkl\".format(out_name), \"wb\") as fp:\n",
    "    #     pickle.dump(stat_dict, fp)\n",
    "\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_train.npy\")\n",
    "test_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculates the statistics of training data. Done!\n"
     ]
    }
   ],
   "source": [
    "stat_dict=generate_stat_from_train_data(df_train, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df[:, :, 0] = (train_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "train_df[:, :, 1] = (train_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[:, :, 0] = (test_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "test_df[:, :, 1] = (test_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # 将经纬度从度转换为弧度\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine 公式\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    \n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c  # 地球半径的平均半径，单位公里\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(latitudes, longitudes):\n",
    "    num_sites = len(latitudes)\n",
    "    distance_matrix = np.zeros((num_sites, num_sites))\n",
    "    \n",
    "    for i in range(num_sites):\n",
    "        for j in range(i + 1, num_sites):\n",
    "            dist = haversine(latitudes[i], longitudes[i], latitudes[j], longitudes[j])\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist  # 因为距离是对称的\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index_from_nearest_neighbors(distance_matrix, num_neighbors=3):\n",
    "    num_sites = distance_matrix.shape[0]\n",
    "    edge_index = []\n",
    "\n",
    "    for i in range(num_sites):\n",
    "        # 获取与站点 i 距离最近的 num_neighbors 个站点的索引\n",
    "        neighbors_indices = np.argsort(distance_matrix[i])[:num_neighbors + 1]  # +1 因为最近的包括自己\n",
    "        for j in neighbors_indices:\n",
    "            if i != j:  # 避免自连接\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "    return torch.tensor(edge_index).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_train,latitudes_train=df_train[\"lon\"].values, df_train[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_train = compute_distance_matrix(longitudes_train, latitudes_train)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_train = create_edge_index_from_nearest_neighbors(distance_matrix_train, num_neighbors=3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_test,latitudes_test=df_test[\"lon\"].values, df_test[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_test = compute_distance_matrix(longitudes_test, latitudes_test)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_test = create_edge_index_from_nearest_neighbors(distance_matrix_test, num_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建最后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "test={\"test_data\":test_data,\"fusion_data\":test_fusion_data,\"invalid_masks\":test_invid_mask,\"test_masks\":test_mask,\"r_pos_mat\":test_df,\"timestamps\":timestap_result,\"edg_index\":edge_index_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "train={\"train_data\":train_data,\"fusion_data\":train_fusion_data,\"invalid_masks\":train_invid_mask,\"r_pos_mat\":train_df,\"timestamps\":timestap_result,\"edg_index\":edge_index_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_data', 'fusion_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps', 'edg_index']\n"
     ]
    }
   ],
   "source": [
    "print(list(test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_data', 'fusion_data', 'invalid_masks', 'r_pos_mat', 'timestamps', 'edg_index']\n"
     ]
    }
   ],
   "source": [
    "print(list(train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_pickle(\"2012-2014_data_train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_bw_train = pd.read_pickle(\"2012-2014_data_bw_train.pkl\")\n",
    "# data_bw_test = pd.read_pickle(\"2012-2014_data_bw_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_bw_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 132, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_bw_train[\"r_pos_mat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 41)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[\"invalid_masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 9, 52, 1)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"train_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,5:,:,:]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_data', 'invalid_masks', 'timestamps', 'r_pos_mat'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('2012-2014_data.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3640, 106, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=data[\"train_data\"]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 63, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.      ],\n",
       "       [221.      ],\n",
       "       [185.      ],\n",
       "       [267.      ],\n",
       "       [119.      ],\n",
       "       [303.      ],\n",
       "       [313.      ],\n",
       "       [ 73.      ],\n",
       "       [ 81.      ],\n",
       "       [119.      ],\n",
       "       [ 91.      ],\n",
       "       [102.      ],\n",
       "       [196.      ],\n",
       "       [240.      ],\n",
       "       [133.      ],\n",
       "       [ 72.      ],\n",
       "       [138.      ],\n",
       "       [135.      ],\n",
       "       [192.      ],\n",
       "       [155.      ],\n",
       "       [206.      ],\n",
       "       [177.      ],\n",
       "       [ 87.      ],\n",
       "       [110.      ],\n",
       "       [ 66.      ],\n",
       "       [ 87.      ],\n",
       "       [ 80.      ],\n",
       "       [ 71.      ],\n",
       "       [ 69.      ],\n",
       "       [296.      ],\n",
       "       [344.      ],\n",
       "       [301.      ],\n",
       "       [291.      ],\n",
       "       [154.      ],\n",
       "       [  0.      ],\n",
       "       [312.      ],\n",
       "       [210.      ],\n",
       "       [280.      ],\n",
       "       [  0.      ],\n",
       "       [310.      ],\n",
       "       [350.      ],\n",
       "       [340.      ],\n",
       "       [360.      ],\n",
       "       [340.      ],\n",
       "       [205.53244 ],\n",
       "       [140.      ],\n",
       "       [282.      ],\n",
       "       [100.      ],\n",
       "       [ 11.      ],\n",
       "       [  0.      ],\n",
       "       [  0.      ],\n",
       "       [332.      ],\n",
       "       [346.      ],\n",
       "       [ 20.      ],\n",
       "       [333.      ],\n",
       "       [  0.      ],\n",
       "       [  0.      ],\n",
       "       [266.      ],\n",
       "       [336.      ],\n",
       "       [ 96.      ],\n",
       "       [286.      ],\n",
       "       [110.      ],\n",
       "       [274.809157]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4],\n",
       "       [2.9],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [1.4],\n",
       "       [1.3],\n",
       "       [1. ],\n",
       "       [0. ],\n",
       "       [0.7],\n",
       "       [0.9],\n",
       "       [3.4],\n",
       "       [0. ],\n",
       "       [0.4],\n",
       "       [0.8],\n",
       "       [5.6],\n",
       "       [0. ],\n",
       "       [5. ],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.6],\n",
       "       [0.6],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [3.3],\n",
       "       [0. ],\n",
       "       [0.1],\n",
       "       [0.1],\n",
       "       [0. ],\n",
       "       [1.6],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.3],\n",
       "       [0.1],\n",
       "       [1. ],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [0.8],\n",
       "       [2. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.1],\n",
       "       [0. ],\n",
       "       [0.4],\n",
       "       [0.5],\n",
       "       [2.1],\n",
       "       [0.9],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.5],\n",
       "       [1.7],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.1],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [2.6],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.4],\n",
       "       [1.1],\n",
       "       [0.1],\n",
       "       [3.7],\n",
       "       [1. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.8],\n",
       "       [2.9],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [2.9],\n",
       "       [1.7],\n",
       "       [0. ],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [1.9],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.2],\n",
       "       [0.9],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0.6],\n",
       "       [0. ],\n",
       "       [3.7],\n",
       "       [1.1],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [2.8],\n",
       "       [0. ],\n",
       "       [0.8],\n",
       "       [0. ],\n",
       "       [0.4],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedAFFWithSoftmax(nn.Module):\n",
    "    def __init__(self, num_timestamps=8784, num_features=9, num_sites=103, r=4):\n",
    "        super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "        self.num_timestamps = num_timestamps\n",
    "        self.num_features = num_features\n",
    "        self.num_sites = num_sites\n",
    "        inter_channels = int(num_features // r * num_features)\n",
    "        \n",
    "        # 图卷积网络\n",
    "\n",
    "        self.GCN1=GCNConv(num_timestamps * num_features, inter_channels),\n",
    "        self.batch=nn.BatchNorm2d(inter_channels),\n",
    "        self.relu1=nn.ReLU(inplace=True),\n",
    "        self.GCN2=GCNConv(inter_channels, num_timestamps * num_features),\n",
    "        self.relu2=nn.BatchNorm2d(num_timestamps * num_features),\n",
    "\n",
    "        \n",
    "        # 传统卷积部分调整为一维操作，以处理1D特征数据\n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(num_features, inter_channels, kernel_size=1,stride=1, padding=0),\n",
    "            nn.BatchNorm1d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(inter_channels, num_features, kernel_size=1,stride=1, padding=0),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "        )\n",
    "\n",
    "        self.mlp=nn.Linear(2,1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 调整x的形状以适应图卷积网络的输入\n",
    "        x=x.squeeze(-1).permute(2, 1, 0)\n",
    "        x_gcn = x.reshape(self.num_sites, -1)  # (103, 8784*9)\n",
    "\n",
    "        # GCN 处理\n",
    "        x_gcn=self.GCN1(x_gcn, edge_index)\n",
    "        x_gcn=self.batch(x_gcn)\n",
    "        x_gcn=self.relu1(x_gcn)\n",
    "        x_gcn=self.GCN2(x_gcn, edge_index)\n",
    "        xl=self.relu2(x_gcn)\n",
    "        \n",
    "        # 调整为原始时间戳维度\n",
    "        x_gcn = x_gcn.view(self.num_sites,  self.num_features,self.num_timestamps).permute(2, 1, 0).unsqueeze(-1)  # (8784, 9, 103, 1)\n",
    "\n",
    "        # Global attention 处理\n",
    "        xg = self.global_att(x)\n",
    "        repeat_num=xl.size(2)\n",
    "        xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "        xlg = torch.cat((xl, xg), dim=3)\n",
    "        batch_size, num_channels, height, width = xlg.size()\n",
    "        xlg = xlg.view(-1, width)\n",
    "        xlg = F.relu(self.mlp(xlg))\n",
    "        xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "        # x_global = x.squeeze(-1).permute(0, 2, 1)  # 形状变为 (8784, 103, 9)\n",
    "        # x_global = self.global_att(x_global)  # 形状: (8784, 9, 103, 1)\n",
    "        # x_global = x_global.permute(0, 2, 1).unsqueeze(-1)\n",
    "\n",
    "        # xl = self.local_att(x)\n",
    "        # xg = self.global_att(x)\n",
    "        # repeat_num=xl.size(2)\n",
    "        # xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "        # xlg = torch.cat((xl, xg), dim=3)\n",
    "        # batch_size, num_channels, height, width = xlg.size()\n",
    "        # xlg = xlg.view(-1, width)\n",
    "        # xlg = F.relu(self.mlp(xlg))\n",
    "        # xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "\n",
    "        # Combine and process weights\n",
    "        # x_combined = x_gcn + x_global\n",
    "        weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = weights.view_as(xlg)\n",
    "        \n",
    "        separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "        weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "        output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "        return output.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceshi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('2012-2014_data_train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# train=data[\"fusion_data\"]\n",
    "# train[0]\n",
    "# train=data[\"fusion_data\"]\n",
    "# train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "# train_data_expanded.shape\n",
    "# train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "# train_data_expanded.shape\n",
    "# train_data=train_data_expanded[:,5:,:,:]\n",
    "# edg_index_in=data[\"edg_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3640, 106, 1)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3640, 106, 1)"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('2012-2014_data_test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False,  True, False, False, False, False,\n",
       "       False, False,  True,  True,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True,  True, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False,  True, False, False, False,  True, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"test_masks\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ceshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 9, 52, 1)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 156])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edg_index_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,5:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8784, 71, 1])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,5:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class ModifiedAFFWithSoftmax(nn.Module):\n",
    "    def __init__(self, num_timestamps=8784, num_features=9, r=4):\n",
    "        super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "        self.num_timestamps = num_timestamps\n",
    "        self.num_features = num_features\n",
    "        # self.num_sites = num_sites\n",
    "        inter_channels = int(num_features // r * num_timestamps)\n",
    "        \n",
    "        # 图卷积网络\n",
    "\n",
    "        self.GCN1=GCNConv(num_timestamps * num_features, inter_channels)\n",
    "        self.batch1=nn.BatchNorm1d(inter_channels)\n",
    "        self.relu1=nn.ReLU(inplace=True)\n",
    "        self.GCN2=GCNConv(inter_channels, num_timestamps * num_features)\n",
    "        self.batch2=nn.BatchNorm1d(num_timestamps * num_features)\n",
    "        # self.relu2=nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 传统卷积部分调整为一维操作，以处理1D特征数据\n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_features, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, num_features, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features),   \n",
    "        )\n",
    "\n",
    "        self.mlp=nn.Linear(2,1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 调整x的形状以适应图卷积网络的输入\n",
    "        num_sites=x.size(2)\n",
    "        x_gcn=x\n",
    "        x_gcn=x_gcn.squeeze(-1).permute(2, 1, 0)\n",
    "        x_gcn = x_gcn.reshape(num_sites, -1)  # (103, 8784*9)\n",
    "\n",
    "\n",
    "        # GCN 处理\n",
    "        x_gcn=self.GCN1(x_gcn, edge_index)\n",
    "        x_gcn=self.batch1(x_gcn)\n",
    "        x_gcn=self.relu1(x_gcn)\n",
    "        x_gcn=self.GCN2(x_gcn, edge_index)\n",
    "        xl=self.batch2(x_gcn)\n",
    "        # xl=self.relu2(x_gcn)\n",
    "        \n",
    "        # 调整为原始时间戳维度\n",
    "        xl = xl.view(num_sites,  self.num_features,self.num_timestamps).permute(2, 1, 0).unsqueeze(-1)  # (8784, 9, 103, 1)\n",
    "\n",
    "        # Global attention 处理\n",
    "        xg = self.global_att(x)\n",
    "        repeat_num=xl.size(2)\n",
    "        xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "        xlg = torch.cat((xl, xg), dim=3)\n",
    "        batch_size, num_channels, height, width = xlg.size()\n",
    "        xlg = xlg.view(-1, width)\n",
    "        xlg = F.relu(self.mlp(xlg))\n",
    "        xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "\n",
    "        weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = weights.view_as(xlg)\n",
    "        \n",
    "        separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "        weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "        output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "        return output.squeeze(1)\n",
    "# class ModifiedAFFWithSoftmax(nn.Module):\n",
    "#     def __init__(self, channels=9, r=4, num_features=9):\n",
    "#         super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "#         self.num_features = num_features\n",
    "#         inter_channels = int(channels // r )\n",
    "        \n",
    "        \n",
    "#         self.local_att = nn.Sequential(\n",
    "#             nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(inter_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(channels),\n",
    "#         )\n",
    "        \n",
    "#         self.global_att = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(inter_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(channels),   \n",
    "#         )\n",
    "\n",
    "#         self.mlp=nn.Linear(2,1)\n",
    "#     def forward(self, x):\n",
    "#         xl = self.local_att(x)\n",
    "#         xg = self.global_att(x)\n",
    "#         repeat_num=xl.size(2)\n",
    "#         xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "#         xlg = torch.cat((xl, xg), dim=3)\n",
    "#         batch_size, num_channels, height, width = xlg.size()\n",
    "#         xlg = xlg.view(-1, width)\n",
    "#         xlg = F.relu(self.mlp(xlg))\n",
    "#         xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "#         # xlg = xl + xg\n",
    "#         # xlg = xlg - xl\n",
    "        \n",
    "#         weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "#         weights = F.softmax(weights, dim=1)\n",
    "#         weights = weights.view_as(xlg)\n",
    "        \n",
    "#         separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "#         weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "#         output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "#         return output.squeeze(1)\n",
    "# 初始化模型\n",
    "model = ModifiedAFFWithSoftmax()\n",
    "\n",
    "# 生成模拟输入数据\n",
    "train_tensor = torch.from_numpy(train_data)  # Batch size 8784, 18 features, 41 sites, width 1\n",
    "train_tensor = train_tensor.float()\n",
    "\n",
    "# 前向传播\n",
    "output_tensor = model(train_tensor,edg_index_in)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8784, 9, 52, 1])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_numpy=output_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32.127617],\n",
       "       [107.00171 ],\n",
       "       [136.66765 ],\n",
       "       [ 93.62737 ],\n",
       "       [126.2027  ],\n",
       "       [ 86.40228 ],\n",
       "       [ 76.56961 ],\n",
       "       [142.7038  ],\n",
       "       [188.48268 ],\n",
       "       [146.61589 ],\n",
       "       [144.86035 ],\n",
       "       [147.1432  ],\n",
       "       [135.56206 ],\n",
       "       [ 95.803604],\n",
       "       [125.354324],\n",
       "       [152.61635 ],\n",
       "       [136.80568 ],\n",
       "       [112.02063 ],\n",
       "       [100.627556],\n",
       "       [125.41099 ],\n",
       "       [100.76597 ],\n",
       "       [ 86.37686 ],\n",
       "       [ 70.158936],\n",
       "       [140.76495 ],\n",
       "       [156.76047 ],\n",
       "       [158.44688 ],\n",
       "       [185.46153 ],\n",
       "       [146.83614 ],\n",
       "       [148.74702 ],\n",
       "       [ 64.645706],\n",
       "       [ 46.472363],\n",
       "       [ 88.016426],\n",
       "       [ 77.6937  ],\n",
       "       [123.21856 ],\n",
       "       [ 56.151684],\n",
       "       [112.73174 ],\n",
       "       [110.40742 ],\n",
       "       [ 93.14596 ],\n",
       "       [102.831436],\n",
       "       [ 59.37419 ],\n",
       "       [ 54.77721 ],\n",
       "       [ 49.511074],\n",
       "       [ 44.447273],\n",
       "       [ 53.370037],\n",
       "       [115.65097 ],\n",
       "       [117.68736 ],\n",
       "       [ 97.92604 ],\n",
       "       [109.574524],\n",
       "       [112.59993 ],\n",
       "       [ 94.198616],\n",
       "       [ 32.127617],\n",
       "       [111.40008 ],\n",
       "       [109.70893 ],\n",
       "       [111.628525],\n",
       "       [ 61.21606 ],\n",
       "       [ 96.47967 ],\n",
       "       [ 21.217356],\n",
       "       [102.00872 ],\n",
       "       [114.2517  ],\n",
       "       [ 58.63027 ],\n",
       "       [ 57.728443],\n",
       "       [164.19138 ],\n",
       "       [103.78823 ]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_numpy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[\"train_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.      ],\n",
       "       [221.      ],\n",
       "       [185.      ],\n",
       "       [267.      ],\n",
       "       [119.      ],\n",
       "       [303.      ],\n",
       "       [313.      ],\n",
       "       [ 73.      ],\n",
       "       [ 81.      ],\n",
       "       [119.      ],\n",
       "       [ 91.      ],\n",
       "       [102.      ],\n",
       "       [196.      ],\n",
       "       [240.      ],\n",
       "       [133.      ],\n",
       "       [ 72.      ],\n",
       "       [138.      ],\n",
       "       [135.      ],\n",
       "       [192.      ],\n",
       "       [155.      ],\n",
       "       [206.      ],\n",
       "       [177.      ],\n",
       "       [ 87.      ],\n",
       "       [110.      ],\n",
       "       [ 66.      ],\n",
       "       [ 87.      ],\n",
       "       [ 80.      ],\n",
       "       [ 71.      ],\n",
       "       [ 69.      ],\n",
       "       [296.      ],\n",
       "       [344.      ],\n",
       "       [301.      ],\n",
       "       [291.      ],\n",
       "       [154.      ],\n",
       "       [  0.      ],\n",
       "       [312.      ],\n",
       "       [210.      ],\n",
       "       [280.      ],\n",
       "       [  0.      ],\n",
       "       [310.      ],\n",
       "       [350.      ],\n",
       "       [340.      ],\n",
       "       [360.      ],\n",
       "       [340.      ],\n",
       "       [205.53244 ],\n",
       "       [140.      ],\n",
       "       [282.      ],\n",
       "       [100.      ],\n",
       "       [ 11.      ],\n",
       "       [  0.      ],\n",
       "       [  0.      ],\n",
       "       [332.      ],\n",
       "       [346.      ],\n",
       "       [ 20.      ],\n",
       "       [333.      ],\n",
       "       [  0.      ],\n",
       "       [  0.      ],\n",
       "       [266.      ],\n",
       "       [336.      ],\n",
       "       [ 96.      ],\n",
       "       [286.      ],\n",
       "       [110.      ],\n",
       "       [274.809157]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 63, 18)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=data[\"fusion_data\"]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "        # load data\n",
    "        with open(path, \"rb\") as fp:\n",
    "            data_dict = pickle.load(fp)\n",
    "\n",
    "        all_seq_data = data_dict[\"train_data\"][:, :, 0:1]\n",
    "        invalid_masks_data = data_dict[\"invalid_masks\"]\n",
    "        r_pos_mat_data = data_dict[\"r_pos_mat\"]\n",
    "\n",
    "        if \"adj_attn_mask\" in data_dict.keys():\n",
    "            adj_attn_mask = data_dict[\"adj_attn_mask\"]\n",
    "        else:\n",
    "            adj_attn_mask = None\n",
    "\n",
    "        return all_seq_data, invalid_masks_data, r_pos_mat_data, adj_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_data, invalid_masks_data, r_pos_mat_data, adj_attn_mask = load_train_data('train.pkl')\n",
    "d_feat, d_pos = all_seq_data[0].shape[-1], r_pos_mat_data[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 63, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seq_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 63)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_masks_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 63, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pos_mat_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def create_train_data(epoch, all_seq_data, invalid_masks, batch_size, masked_lm_prob, times=1, adj_attn_mask=None):\n",
    "    \"\"\"times: create how many times of the whole data\"\"\"\n",
    "\n",
    "    data_num = len(all_seq_data)\n",
    "    seq_len = all_seq_data.shape[1]\n",
    "    all_indexes = list(range(data_num))\n",
    "\n",
    "    max_pred_per_seq = int(seq_len * masked_lm_prob)\n",
    "\n",
    "    batch_num = data_num // batch_size + 1\n",
    "\n",
    "    with tqdm(total=batch_num*times, desc=f'Epoch {epoch}:') as pbar:\n",
    "        for t in range(times):\n",
    "            random.shuffle(all_indexes)  # for each masked round, shuffle the whole data\n",
    "\n",
    "            for i in range(batch_num):\n",
    "                end_idx = min(data_num, (i + 1) * batch_size)\n",
    "                batch_indexes = all_indexes[i * batch_size: end_idx]\n",
    "\n",
    "                masked_seq_list, masked_idx_list = [], []\n",
    "                masked_labels_list, masked_label_weights_list, attn_mask_list = [], [], []\n",
    "                for idx in batch_indexes:  # todo: modify batch, concatenate\n",
    "                    seq_data = all_seq_data[idx]\n",
    "\n",
    "                    if invalid_masks is not None:\n",
    "                        invalid_mask = invalid_masks[idx]\n",
    "                    else:\n",
    "                        invalid_mask = None\n",
    "\n",
    "                    masked_seq, masked_indexes, masked_labels, masked_label_weights, attn_mask = \\\n",
    "                        create_random_masked_data(seq_data, seq_len, invalid_mask, max_pred_per_seq, masked_lm_prob)\n",
    "\n",
    "                    if adj_attn_mask is not None:\n",
    "                        attn_mask = np.logical_and(attn_mask, adj_attn_mask).astype(int)\n",
    "\n",
    "                    if masked_seq is not None:\n",
    "                        masked_seq_list.append(masked_seq)\n",
    "                        masked_idx_list.append(masked_indexes)\n",
    "                        masked_labels_list.append(masked_labels)\n",
    "                        masked_label_weights_list.append(masked_label_weights)\n",
    "                        attn_mask_list.append(attn_mask)\n",
    "\n",
    "                masked_seq_arr = np.array(masked_seq_list)\n",
    "                masked_idx_arr = np.array(masked_idx_list)\n",
    "                masked_labels_arr = np.array(masked_labels_list)\n",
    "                masked_label_weights_arr = np.array(masked_label_weights_list)\n",
    "                attn_mask_arr = np.array(attn_mask_list)\n",
    "\n",
    "                # When all masked_seq with zero std, will generate empty batch\n",
    "                # if masked_seq_arr.size != 0:\n",
    "                yield [masked_seq_arr, masked_idx_arr, masked_labels_arr, masked_label_weights_arr, attn_mask_arr]\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "def create_random_masked_data(seq_data, seq_len, invalid_mask, max_pred_per_seq, masked_lm_prob=0.15):\n",
    "    masked_seq = seq_data.copy()\n",
    "    full_indexes = set(range(seq_len))\n",
    "\n",
    "    if invalid_mask is not None:\n",
    "        invalid_indexes = set(np.where(invalid_mask)[0])\n",
    "    else:\n",
    "        invalid_indexes = set()\n",
    "\n",
    "    valid_indexes = full_indexes - invalid_indexes\n",
    "    if len(valid_indexes) < 5:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # masked_lm_prob = random.uniform(0.1, 0.3)\n",
    "    n_pred = min(max_pred_per_seq, max(1, int(round(len(valid_indexes) * masked_lm_prob))))\n",
    "    masked_indexes = random.sample((list(valid_indexes)), n_pred)\n",
    "    masked_labels = list(seq_data[masked_indexes, :1])\n",
    "\n",
    "    unmasked_indexes = list(valid_indexes - set(masked_indexes))\n",
    "    mean_value = np.mean(seq_data[unmasked_indexes, :1])\n",
    "    std_value = np.std(seq_data[unmasked_indexes, :1])\n",
    "\n",
    "    # if std_value == 0:  # Discard the seqs: after normalization, will generate nan or inf value\n",
    "    #     return None, None, None, None, None\n",
    "    # masked_seq = (masked_seq - mean_value) / std_value  # normalization\n",
    "    # masked_seq[masked_indexes, :1] = 0  # set masked nodes as 0\n",
    "\n",
    "    # fixme: use this all zero sequence\n",
    "    if std_value == 0:  # Discard the seqs: after normalization, will generate nan or inf value\n",
    "        masked_seq = masked_seq - mean_value  # normalization\n",
    "    else:\n",
    "        masked_seq = (masked_seq - mean_value) / std_value  # normalization\n",
    "    masked_seq[masked_indexes, :1] = 0  # set masked nodes as 0; this 0 denotes the mean value\n",
    "\n",
    "    masked_label_weights = [1.0] * len(masked_labels)\n",
    "    if max_pred_per_seq > n_pred:\n",
    "        n_pad = max_pred_per_seq - n_pred\n",
    "        masked_labels.extend([[0]] * n_pad)\n",
    "        masked_indexes.extend([0] * n_pad)\n",
    "        masked_label_weights.extend([0] * n_pad)\n",
    "\n",
    "    attn_mask = get_attn_mask(seq_len, unmasked_indexes)\n",
    "\n",
    "    masked_indexes = np.array(masked_indexes).astype(int)\n",
    "    masked_label_weights = np.array(masked_label_weights).astype(float)\n",
    "    attn_mask = np.array(attn_mask).astype(int)\n",
    "\n",
    "    # fixme: std = 0\n",
    "    # masked_labels = (np.array(masked_labels).astype(float) - mean_value) / std_value  # standardize label\n",
    "    if std_value == 0:  # Discard the seqs: after normalization, will generate nan or inf value\n",
    "        masked_labels = np.array(masked_labels).astype(float) - mean_value  # standardize label\n",
    "    else:\n",
    "        masked_labels = (np.array(masked_labels).astype(float) - mean_value) / std_value  # standardize label\n",
    "\n",
    "    return masked_seq, masked_indexes, masked_labels, masked_label_weights, attn_mask\n",
    "def get_attn_mask(max_seq_len, unmasked_indexes):\n",
    "    attn_vec = np.zeros(max_seq_len)\n",
    "\n",
    "    # for each node, cut off the edge between it and invalid (masked or padded) nodes\n",
    "    attn_vec[unmasked_indexes] = 1  # vector\n",
    "    attn_mask = np.tile(attn_vec, (max_seq_len, 1))  # n*n matrix:\n",
    "\n",
    "    # for each node (mainly for masked nodes), add itself edge for attention calculation\n",
    "    eye_mask = np.eye(max_seq_len)\n",
    "\n",
    "    attn_mask = np.logical_or(attn_mask, eye_mask).astype(int)\n",
    "\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_generator(self):\n",
    "        # load data\n",
    "        with open(self.args.test_data_path, \"rb\") as fp:\n",
    "            data_dict = pickle.load(fp)\n",
    "\n",
    "        all_seq_data = data_dict[\"test_data\"][:, :, 0:1]\n",
    "        r_pos_mat = data_dict[\"r_pos_mat\"]\n",
    "        invalid_masks = data_dict[\"invalid_masks\"]\n",
    "        test_masks = data_dict[\"test_masks\"]\n",
    "        all_timestamps = data_dict[\"timestamps\"]\n",
    "\n",
    "        if \"adj_attn_mask\" in data_dict.keys():\n",
    "            adj_attn_mask = data_dict[\"adj_attn_mask\"]\n",
    "        else:\n",
    "            adj_attn_mask = None\n",
    "\n",
    "        test_data_generator = create_test_data(all_seq_data, invalid_masks, test_masks, all_timestamps, adj_attn_mask)\n",
    "\n",
    "        return r_pos_mat, test_data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gcn fusion 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedAFFWithSoftmax(nn.Module):\n",
    "    def __init__(self, num_timestamps=8784, num_features=9, num_sites=103, r=4):\n",
    "        super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "        self.num_timestamps = num_timestamps\n",
    "        self.num_features = num_features\n",
    "        self.num_sites = num_sites\n",
    "        inter_channels = int(num_features // r * num_sites)\n",
    "        \n",
    "        # 图卷积网络\n",
    "        self.gcn1 = GCNConv(num_timestamps * num_features, inter_channels)\n",
    "        self.gcn2 = GCNConv(inter_channels, num_timestamps)\n",
    "        \n",
    "        # 传统卷积部分调整为一维操作，以处理1D特征数据\n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(num_features, inter_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(inter_channels, num_features, kernel_size=1),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 调整x的形状以适应图卷积网络的输入\n",
    "        x_gcn = x.permute(2, 0, 1).reshape(self.num_sites, -1)  # (103, 8784*9)\n",
    "\n",
    "        # GCN 处理\n",
    "        x_gcn = F.relu(self.gcn1(x_gcn, edge_index))\n",
    "        x_gcn = self.gcn2(x_gcn, edge_index)  # 输出形状: (103, 8784)\n",
    "        \n",
    "        # 调整为原始时间戳维度\n",
    "        x_gcn = x_gcn.view(self.num_sites, self.num_timestamps, self.num_features).permute(1, 2, 0).unsqueeze(-1)  # (8784, 9, 103, 1)\n",
    "\n",
    "        # Global attention 处理\n",
    "        x_global = x.squeeze(-1).permute(0, 2, 1)  # 形状变为 (8784, 103, 9)\n",
    "        x_global = self.global_att(x_global)  # 形状: (8784, 9, 103, 1)\n",
    "        x_global = x_global.permute(0, 2, 1).unsqueeze(-1)\n",
    "\n",
    "        # Combine and process weights\n",
    "        x_combined = x_gcn + x_global\n",
    "        weights = F.softmax(x_combined.squeeze(-1), dim=2)  # Softmax over sites dimension\n",
    "        output = weights * x  # Element-wise multiplication for weighting\n",
    "        output = output.sum(dim=2, keepdim=True)  # Sum over features\n",
    "\n",
    "        return output  # 形状为 (8784, 103, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
