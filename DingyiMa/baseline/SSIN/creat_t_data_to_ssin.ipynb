{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from geographiclib.geodesic import Geodesic\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=np.load('D:\\Spatial_interpolation\\SSIN\\data\\Te_Hiku/all.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141, 7, 24)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=np.array(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=6\n",
    "train_data=X_raw[:,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141, 24)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.7, 20.5, 21.4, ..., 21. , 19.9, 19. ],\n",
       "       [19.7, 20.5, 21.4, ..., 21. , 19.9, 19. ],\n",
       "       [19.7, 20.5, 21.4, ..., 21. , 19.9, 19. ],\n",
       "       ...,\n",
       "       [14.6, 15. , 15.7, ..., 15.5, 14.3, 14.5],\n",
       "       [14.6, 15. , 15.7, ..., 15.6, 14.3, 14.5],\n",
       "       [14.6, 15. , 15.8, ..., 15.6, 14.3, 14.5]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "missing_columns = np.where(columns_with_missing_data)[0]\n",
    "result = np.delete(train_data, missing_columns, axis=1)\n",
    "result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10], dtype=int64)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=np.array(result,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141, 23)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "test_list=[16,17,18,19,20,21,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=result[:,train_list]\n",
    "test_data=result[:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(train_data,dtype=float)\n",
    "test_data=np.array(test_data,dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fusion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=np.delete(X_raw, missing_columns, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=train_fusion_data[:,2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=np.array(train_fusion_data,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_index in range(0, train_fusion_data.shape[1]):\n",
    "    feature_column = train_fusion_data[:, feature_index, :]\n",
    "    min=np.nanmin(feature_column)\n",
    "    max=np.nanmax(feature_column)\n",
    "    norm_data=(feature_column - min) / (max - min)\n",
    "    train_fusion_data[:, feature_index, :]=norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data[train_fusion_data == 0] = 0.00001\n",
    "train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141, 5, 23)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=train_fusion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "test_list=[16,17,18,19,20,21,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=result[:,:,train_list]\n",
    "test_fusion_data=result[:,:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data = np.transpose(train_fusion_data, (0, 2, 1))\n",
    "test_fusion_data = np.transpose(test_fusion_data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=np.array(train_fusion_data,dtype=float)\n",
    "test_fusion_data=np.array(test_fusion_data,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02695418, 0.04237288, 0.0880829 , 0.14138817, 0.52531646],\n",
       "        [0.06469003, 0.10734463, 0.11658031, 0.08997429, 0.57594937],\n",
       "        [0.        , 0.08474576, 0.12435233, 0.10796915, 0.63291139],\n",
       "        ...,\n",
       "        [0.02425876, 0.03954802, 0.17098446, 0.76092545, 0.48734177],\n",
       "        [0.11051213, 0.09039548, 0.04663212, 0.09254499, 0.55063291],\n",
       "        [0.01347709, 0.01129944, 0.08290155, 0.22107969, 0.5443038 ]],\n",
       "\n",
       "       [[0.02695418, 0.04237288, 0.0880829 , 0.14395887, 0.52531646],\n",
       "        [0.06469003, 0.10734463, 0.11658031, 0.08997429, 0.57594937],\n",
       "        [0.        , 0.08474576, 0.12176166, 0.10796915, 0.63291139],\n",
       "        ...,\n",
       "        [0.02695418, 0.03954802, 0.17098446, 0.76092545, 0.48734177],\n",
       "        [0.11051213, 0.09039548, 0.04663212, 0.09254499, 0.55063291],\n",
       "        [0.01347709, 0.01129944, 0.08290155, 0.22107969, 0.5443038 ]],\n",
       "\n",
       "       [[0.02695418, 0.04237288, 0.0880829 , 0.14395887, 0.52531646],\n",
       "        [0.06469003, 0.10734463, 0.11658031, 0.08997429, 0.57594937],\n",
       "        [0.        , 0.08757062, 0.12176166, 0.10796915, 0.63291139],\n",
       "        ...,\n",
       "        [0.02425876, 0.03954802, 0.17098446, 0.76092545, 0.48734177],\n",
       "        [0.10781671, 0.09039548, 0.04663212, 0.09254499, 0.55063291],\n",
       "        [0.01347709, 0.01129944, 0.08290155, 0.22107969, 0.5443038 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.0458221 , 0.16384181, 0.1761658 , 0.2596401 , 0.20253165],\n",
       "        [0.21563342, 0.22033898, 0.20466321, 0.04627249, 0.2278481 ],\n",
       "        [0.15633423, 0.23163842, 0.21502591, 0.1722365 , 0.2721519 ],\n",
       "        ...,\n",
       "        [0.13477089, 0.26553672, 0.84715026, 0.96143959, 0.24050633],\n",
       "        [0.20754717, 0.18079096, 0.19689119, 0.218509  , 0.2278481 ],\n",
       "        [0.12938005, 0.13841808, 0.25647668, 0.8277635 , 0.26582278]],\n",
       "\n",
       "       [[0.0458221 , 0.16384181, 0.1761658 , 0.2596401 , 0.20253165],\n",
       "        [0.21563342, 0.22033898, 0.20207254, 0.04627249, 0.2278481 ],\n",
       "        [0.15633423, 0.23163842, 0.21502591, 0.16966581, 0.2721519 ],\n",
       "        ...,\n",
       "        [0.13477089, 0.26553672, 0.84715026, 0.96143959, 0.24050633],\n",
       "        [0.20754717, 0.18079096, 0.19689119, 0.2159383 , 0.2278481 ],\n",
       "        [0.13477089, 0.14689266, 0.2642487 , 0.84832905, 0.26582278]],\n",
       "\n",
       "       [[0.0458221 , 0.16384181, 0.1761658 , 0.2596401 , 0.20253165],\n",
       "        [0.21563342, 0.22033898, 0.20207254, 0.04627249, 0.2278481 ],\n",
       "        [0.15633423, 0.23163842, 0.21502591, 0.16966581, 0.27848101],\n",
       "        ...,\n",
       "        [0.13477089, 0.26553672, 0.84715026, 0.96143959, 0.24050633],\n",
       "        [0.21024259, 0.18079096, 0.19430052, 0.2159383 , 0.23417722],\n",
       "        [0.12938005, 0.13841808, 0.25647668, 0.8277635 , 0.26582278]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_list=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=X_raw[:,timestap_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap.astype(np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timestap)):\n",
    "    for n in range(len(timestap[i][0])):\n",
    "        timestap[i,0,n]=timestap[i,0,n].replace(\"-\",\"\")\n",
    "        timestap[i,1,n]=timestap[i,1,n].replace(\":\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap[:,:,1]\n",
    "timestap_result=[]\n",
    "for i in range(len(timestap)):\n",
    "    x=timestap[i,0]+\"-\"+timestap[i,1]\n",
    "    timestap_result.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_result=np.array(timestap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141,)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestap_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20211209-102000', '20211209-102500', '20211209-103000', ...,\n",
       "       '20220923-131500', '20220923-132000', '20220923-132500'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invid_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(False)\n",
    "    return np.array(array)\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "train_invid_mask=[]\n",
    "test_invid_mask=[]\n",
    "for i in range(len(timestap)):\n",
    "    random_array = generate1_random_array(len(train_data[0]))\n",
    "    random_array1 = generate1_random_array(len(test_data[0]))\n",
    "    train_invid_mask.append(random_array)\n",
    "    test_invid_mask.append(random_array1)\n",
    "train_invid_mask=np.array(train_invid_mask)\n",
    "test_invid_mask=np.array(test_invid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "            array.append(random.choice([False,True]))\n",
    "    return np.array(array)\n",
    "\n",
    "# 生成长度为10的随机数组\n",
    "test_mask=[]\n",
    "random_array=generate_random_array(len(test_data[0]))\n",
    "for i in range(len(timestap)):\n",
    "    test_mask.append(random_array)\n",
    "test_mask=np.array(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 地理位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(\"D:\\Spatial_interpolation\\SSIN\\data\\Te_Hiku/Tehiku_coordinate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>...</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P24</th>\n",
       "      <th>P25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alt</td>\n",
       "      <td>62.606228</td>\n",
       "      <td>69.289428</td>\n",
       "      <td>81.986228</td>\n",
       "      <td>68.399528</td>\n",
       "      <td>78.680578</td>\n",
       "      <td>63.888428</td>\n",
       "      <td>81.039978</td>\n",
       "      <td>60.922928</td>\n",
       "      <td>81.453778</td>\n",
       "      <td>...</td>\n",
       "      <td>66.715378</td>\n",
       "      <td>81.986228</td>\n",
       "      <td>67.662078</td>\n",
       "      <td>67.514528</td>\n",
       "      <td>65.766678</td>\n",
       "      <td>84.923778</td>\n",
       "      <td>75.129478</td>\n",
       "      <td>82.146528</td>\n",
       "      <td>60.111478</td>\n",
       "      <td>59.956428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>173.106350</td>\n",
       "      <td>173.126400</td>\n",
       "      <td>173.142486</td>\n",
       "      <td>173.115715</td>\n",
       "      <td>173.132170</td>\n",
       "      <td>173.118677</td>\n",
       "      <td>173.148900</td>\n",
       "      <td>173.108604</td>\n",
       "      <td>173.146267</td>\n",
       "      <td>...</td>\n",
       "      <td>173.113154</td>\n",
       "      <td>173.142486</td>\n",
       "      <td>173.113248</td>\n",
       "      <td>173.118065</td>\n",
       "      <td>173.119353</td>\n",
       "      <td>173.123860</td>\n",
       "      <td>173.132426</td>\n",
       "      <td>173.133780</td>\n",
       "      <td>173.122335</td>\n",
       "      <td>173.116352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lat</td>\n",
       "      <td>-34.913204</td>\n",
       "      <td>-34.923743</td>\n",
       "      <td>-34.904647</td>\n",
       "      <td>-34.914770</td>\n",
       "      <td>-34.930635</td>\n",
       "      <td>-34.927594</td>\n",
       "      <td>-34.916512</td>\n",
       "      <td>-34.912781</td>\n",
       "      <td>-34.908781</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.922945</td>\n",
       "      <td>-34.904647</td>\n",
       "      <td>-34.924544</td>\n",
       "      <td>-34.923364</td>\n",
       "      <td>-34.922961</td>\n",
       "      <td>-34.918254</td>\n",
       "      <td>-34.913293</td>\n",
       "      <td>-34.913681</td>\n",
       "      <td>-34.928972</td>\n",
       "      <td>-34.929476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0          P1          P2          P3          P4          P5  \\\n",
       "0        Alt   62.606228   69.289428   81.986228   68.399528   78.680578   \n",
       "1       long  173.106350  173.126400  173.142486  173.115715  173.132170   \n",
       "2        Lat  -34.913204  -34.923743  -34.904647  -34.914770  -34.930635   \n",
       "\n",
       "           P6          P7          P8          P9  ...         P15  \\\n",
       "0   63.888428   81.039978   60.922928   81.453778  ...   66.715378   \n",
       "1  173.118677  173.148900  173.108604  173.146267  ...  173.113154   \n",
       "2  -34.927594  -34.916512  -34.912781  -34.908781  ...  -34.922945   \n",
       "\n",
       "          P16         P17         P18         P20         P21         P22  \\\n",
       "0   81.986228   67.662078   67.514528   65.766678   84.923778   75.129478   \n",
       "1  173.142486  173.113248  173.118065  173.119353  173.123860  173.132426   \n",
       "2  -34.904647  -34.924544  -34.923364  -34.922961  -34.918254  -34.913293   \n",
       "\n",
       "          P23         P24         P25  \n",
       "0   82.146528   60.111478   59.956428  \n",
       "1  173.133780  173.122335  173.116352  \n",
       "2  -34.913681  -34.928972  -34.929476  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=stations.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=stations.iloc[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations.rename(columns={1: 'lon', 2: 'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>173.10635</td>\n",
       "      <td>-34.913204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>173.1264</td>\n",
       "      <td>-34.923743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>173.142486</td>\n",
       "      <td>-34.904647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>173.115715</td>\n",
       "      <td>-34.91477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>173.13217</td>\n",
       "      <td>-34.930635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>173.118677</td>\n",
       "      <td>-34.927594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P7</th>\n",
       "      <td>173.1489</td>\n",
       "      <td>-34.916512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P8</th>\n",
       "      <td>173.108604</td>\n",
       "      <td>-34.912781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9</th>\n",
       "      <td>173.146267</td>\n",
       "      <td>-34.908781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P10</th>\n",
       "      <td>173.14545</td>\n",
       "      <td>-34.907938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P11</th>\n",
       "      <td>173.136643</td>\n",
       "      <td>-34.933227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P12</th>\n",
       "      <td>173.126903</td>\n",
       "      <td>-34.924099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P13</th>\n",
       "      <td>173.135672</td>\n",
       "      <td>-34.91736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P14</th>\n",
       "      <td>173.107295</td>\n",
       "      <td>-34.913848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P15</th>\n",
       "      <td>173.113154</td>\n",
       "      <td>-34.922945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P16</th>\n",
       "      <td>173.142486</td>\n",
       "      <td>-34.904647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P17</th>\n",
       "      <td>173.113248</td>\n",
       "      <td>-34.924544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P18</th>\n",
       "      <td>173.118065</td>\n",
       "      <td>-34.923364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P20</th>\n",
       "      <td>173.119353</td>\n",
       "      <td>-34.922961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P21</th>\n",
       "      <td>173.12386</td>\n",
       "      <td>-34.918254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P22</th>\n",
       "      <td>173.132426</td>\n",
       "      <td>-34.913293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P23</th>\n",
       "      <td>173.13378</td>\n",
       "      <td>-34.913681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P24</th>\n",
       "      <td>173.122335</td>\n",
       "      <td>-34.928972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P25</th>\n",
       "      <td>173.116352</td>\n",
       "      <td>-34.929476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lon        lat\n",
       "P1    173.10635 -34.913204\n",
       "P2     173.1264 -34.923743\n",
       "P3   173.142486 -34.904647\n",
       "P4   173.115715  -34.91477\n",
       "P5    173.13217 -34.930635\n",
       "P6   173.118677 -34.927594\n",
       "P7     173.1489 -34.916512\n",
       "P8   173.108604 -34.912781\n",
       "P9   173.146267 -34.908781\n",
       "P10   173.14545 -34.907938\n",
       "P11  173.136643 -34.933227\n",
       "P12  173.126903 -34.924099\n",
       "P13  173.135672  -34.91736\n",
       "P14  173.107295 -34.913848\n",
       "P15  173.113154 -34.922945\n",
       "P16  173.142486 -34.904647\n",
       "P17  173.113248 -34.924544\n",
       "P18  173.118065 -34.923364\n",
       "P20  173.119353 -34.922961\n",
       "P21   173.12386 -34.918254\n",
       "P22  173.132426 -34.913293\n",
       "P23   173.13378 -34.913681\n",
       "P24  173.122335 -34.928972\n",
       "P25  173.116352 -34.929476"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.drop([  \"P11\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=stations.iloc[0:16]\n",
    "df_test=stations.iloc[16:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alienware\\AppData\\Local\\Temp\\ipykernel_94892\\784921970.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['lat']=df_test['lat'].astype(float)\n",
      "C:\\Users\\alienware\\AppData\\Local\\Temp\\ipykernel_94892\\784921970.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['lon']=df_test['lon'].astype(float)\n",
      "C:\\Users\\alienware\\AppData\\Local\\Temp\\ipykernel_94892\\784921970.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['lat']=df_train['lat'].astype(float)\n",
      "C:\\Users\\alienware\\AppData\\Local\\Temp\\ipykernel_94892\\784921970.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['lon']=df_train['lon'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "df_test['lat']=df_test['lat'].astype(float)\n",
    "df_test['lon']=df_test['lon'].astype(float)\n",
    "df_train['lat']=df_train['lat'].astype(float)\n",
    "df_train['lon']=df_train['lon'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_angle_mat(df, out_path):\n",
    "    lons, lats = df[\"lon\"].values, df[\"lat\"].values\n",
    "    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n",
    "\n",
    "    for i in range(len(lons)):\n",
    "        for j in range(len(lons)):\n",
    "            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n",
    "            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n",
    "            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n",
    "\n",
    "    print(dist_angle_mat.shape)\n",
    "    # print(dist_angle_mat)\n",
    "    np.save(out_path, dist_angle_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_train.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_train, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}/BW_132_data\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_test.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_test, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_from_train_data(train_info_df, r_pos_mat):\n",
    "    # info_df = pd.read_csv(info_path)\n",
    "    # ori_r_pos_mat = np.load(relative_pos_mat_path)\n",
    "\n",
    "    # is_test = info_df[\"is_test\"].values\n",
    "    # train_mask = np.where(is_test == 0, True, False)\n",
    "    # train_info_df = info_df.loc[train_mask, :]\n",
    "\n",
    "    lat_mean, lat_std, lat_max, lat_min = train_info_df[\"lat\"].mean(), train_info_df[\"lat\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lat\"].max(), train_info_df[\"lat\"].min()\n",
    "    lon_mean, lon_std, lon_max, lon_min = train_info_df[\"lon\"].mean(), train_info_df[\"lon\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lon\"].max(), train_info_df[\"lon\"].min()\n",
    "\n",
    "    # indexes = np.where(train_mask)[0]\n",
    "    # idx_i, idx_j = np.ix_(indexes, indexes)\n",
    "    # r_pos_mat = ori_r_pos_mat[idx_i, idx_j, :]\n",
    "\n",
    "    r_dist_mat = r_pos_mat[:, :, 0]\n",
    "    r_angle_mat = r_pos_mat[:, :, 1]\n",
    "\n",
    "    r_dist_mean, r_dist_std, r_dist_max, r_dist_min = np.mean(r_dist_mat), np.std(r_dist_mat), \\\n",
    "                                                      np.max(r_dist_mat), np.min(r_dist_mat),\n",
    "    r_angle_mean, r_angle_std, r_angle_max, r_angle_min = np.mean(r_angle_mat), np.std(r_angle_mat), \\\n",
    "                                                          np.max(r_angle_mat), np.min(r_angle_mat)\n",
    "\n",
    "    stat_dict = {}\n",
    "    stat_dict[\"lat_mean\"], stat_dict[\"lat_std\"], stat_dict[\"lat_max\"], stat_dict[\"lat_min\"] = \\\n",
    "        lat_mean, lat_std, lat_max, lat_min\n",
    "    stat_dict[\"lon_mean\"], stat_dict[\"lon_std\"], stat_dict[\"lon_max\"], stat_dict[\"lon_min\"] = \\\n",
    "        lon_mean, lon_std, lon_max, lon_min\n",
    "\n",
    "    stat_dict[\"r_dist_mean\"], stat_dict[\"r_dist_std\"], stat_dict[\"r_dist_max\"], stat_dict[\"r_dist_min\"] = \\\n",
    "        r_dist_mean, r_dist_std, r_dist_max, r_dist_min\n",
    "    stat_dict[\"r_angle_mean\"], stat_dict[\"r_angle_std\"], stat_dict[\"r_angle_max\"], stat_dict[\"r_angle_min\"] = \\\n",
    "        r_angle_mean, r_angle_std, r_angle_max, r_angle_min\n",
    "\n",
    "    print(\"Calculates the statistics of training data. Done!\")\n",
    "\n",
    "    # with open(\"./data/hk_data_stats.pkl\".format(out_name), \"wb\") as fp:\n",
    "    #     pickle.dump(stat_dict, fp)\n",
    "\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_train.npy\")\n",
    "test_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculates the statistics of training data. Done!\n"
     ]
    }
   ],
   "source": [
    "stat_dict=generate_stat_from_train_data(df_train, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:, :, 0] = (train_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "train_df[:, :, 1] = (train_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[:, :, 0] = (test_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "test_df[:, :, 1] = (test_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # 将经纬度从度转换为弧度\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine 公式\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    \n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c  # 地球半径的平均半径，单位公里\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(latitudes, longitudes):\n",
    "    num_sites = len(latitudes)\n",
    "    distance_matrix = np.zeros((num_sites, num_sites))\n",
    "    \n",
    "    for i in range(num_sites):\n",
    "        for j in range(i + 1, num_sites):\n",
    "            dist = haversine(latitudes[i], longitudes[i], latitudes[j], longitudes[j])\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist  # 因为距离是对称的\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index_from_nearest_neighbors(distance_matrix, num_neighbors=3):\n",
    "    num_sites = distance_matrix.shape[0]\n",
    "    edge_index = []\n",
    "\n",
    "    for i in range(num_sites):\n",
    "        # 获取与站点 i 距离最近的 num_neighbors 个站点的索引\n",
    "        neighbors_indices = np.argsort(distance_matrix[i])[:num_neighbors + 1]  # +1 因为最近的包括自己\n",
    "        for j in neighbors_indices:\n",
    "            if i != j:  # 避免自连接\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "    return torch.tensor(edge_index).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_train,latitudes_train=df_train[\"lon\"].values, df_train[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_train = compute_distance_matrix(longitudes_train, latitudes_train)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_train = create_edge_index_from_nearest_neighbors(distance_matrix_train, num_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_test,latitudes_test=df_test[\"lon\"].values, df_test[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_test = compute_distance_matrix(longitudes_test, latitudes_test)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_test = create_edge_index_from_nearest_neighbors(distance_matrix_test, num_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建最后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test={\"test_data\":test_data,\"invalid_masks\":test_invid_mask,\"test_masks\":test_mask,\"r_pos_mat\":test_df,\"timestamps\":timestap_result,\"edg_index\":edge_index_test,\"fusion_data\":test_fusion_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train={\"train_data\":train_data,\"invalid_masks\":train_invid_mask,\"r_pos_mat\":train_df,\"timestamps\":timestap_result,\"edg_index\":edge_index_train,\"fusion_data\":train_fusion_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test={\"test_data\":test[\"test_data\"],\"invalid_masks\":test[\"invalid_masks\"],\"test_masks\":test[\"test_masks\"],\"r_pos_mat\":test[\"r_pos_mat\"],\"timestamps\":test[\"timestamps\"],\"edg_index\":test[\"edg_index\"],\"fusion_data\":test_fusion_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train={\"train_data\":train[\"train_data\"],\"invalid_masks\":train[\"invalid_masks\"],\"r_pos_mat\":train[\"r_pos_mat\"],\"timestamps\":train[\"timestamps\"],\"edg_index\":train[\"edg_index\"],\"fusion_data\":train_fusion_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps', 'edg_index', 'fusion_data']\n"
     ]
    }
   ],
   "source": [
    "print(list(test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141,)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['timestamps'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数组中不存在 NaN 值\n"
     ]
    }
   ],
   "source": [
    "# 找出 NaN 值的位置\n",
    "nan_indices = np.argwhere(np.isnan(train_data))\n",
    "\n",
    "if nan_indices.size > 0:\n",
    "    print(\"数组中存在 NaN 值，位置如下：\")\n",
    "    print(nan_indices)\n",
    "else:\n",
    "    print(\"数组中不存在 NaN 值\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34141, 5, 7, 1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([34141, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class ModifiedAFFWithSoftmax(nn.Module):\n",
    "#     def __init__(self, num_timestamps=34141, num_features=5, r=4):\n",
    "#         super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "#         self.num_timestamps = num_timestamps\n",
    "#         self.num_features = num_features\n",
    "#         # self.num_sites = num_sites\n",
    "#         inter_channels = int(num_features // r * num_timestamps)\n",
    "        \n",
    "#         # 图卷积网络\n",
    "\n",
    "#         self.GCN1=GCNConv(num_timestamps * num_features, inter_channels)\n",
    "#         self.batch1=nn.BatchNorm1d(inter_channels)\n",
    "#         self.relu1=nn.ReLU(inplace=True)\n",
    "#         self.GCN2=GCNConv(inter_channels, num_timestamps * num_features)\n",
    "#         self.batch2=nn.BatchNorm1d(num_timestamps * num_features)\n",
    "#         # self.relu2=nn.ReLU(inplace=True)\n",
    "        \n",
    "#         # 传统卷积部分调整为一维操作，以处理1D特征数据\n",
    "#         self.global_att = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Conv2d(num_features, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(inter_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(inter_channels, num_features, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(num_features),   \n",
    "#         )\n",
    "\n",
    "#         self.mlp=nn.Linear(2,1)\n",
    "\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # 调整x的形状以适应图卷积网络的输入\n",
    "#         num_sites=x.size(2)\n",
    "#         x_gcn=x\n",
    "#         x_gcn=x_gcn.squeeze(-1).permute(2, 1, 0)\n",
    "#         x_gcn = x_gcn.reshape(num_sites, -1)  # (103, 8784*9)\n",
    "\n",
    "\n",
    "#         # GCN 处理\n",
    "#         x_gcn=self.GCN1(x_gcn, edge_index)\n",
    "#         x_gcn=self.batch1(x_gcn)\n",
    "#         x_gcn=self.relu1(x_gcn)\n",
    "#         x_gcn=self.GCN2(x_gcn, edge_index)\n",
    "#         xl=self.batch2(x_gcn)\n",
    "#         # xl=self.relu2(x_gcn)\n",
    "        \n",
    "#         # 调整为原始时间戳维度\n",
    "#         xl = xl.view(num_sites,  self.num_features,self.num_timestamps).permute(2, 1, 0).unsqueeze(-1)  # (8784, 9, 103, 1)\n",
    "\n",
    "#         # Global attention 处理\n",
    "#         xg = self.global_att(x)\n",
    "#         repeat_num=xl.size(2)\n",
    "#         xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "#         xlg = torch.cat((xl, xg), dim=3)\n",
    "#         batch_size, num_channels, height, width = xlg.size()\n",
    "#         xlg = xlg.view(-1, width)\n",
    "#         xlg = F.relu(self.mlp(xlg))\n",
    "#         xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "\n",
    "#         weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "#         weights = F.softmax(weights, dim=1)\n",
    "#         weights = weights.view_as(xlg)\n",
    "        \n",
    "#         separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "#         weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "#         output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "#         return output.squeeze(1)\n",
    "# class ModifiedAFFWithSoftmax(nn.Module):\n",
    "#     def __init__(self, channels=5, r=4, num_features=5):\n",
    "#         super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "#         self.layer1 = nn.Linear(5, 1)\n",
    "#     def forward(self, x):\n",
    "#         batch_size, num_channels, height, width = x.size()\n",
    "#         x=x.squeeze(-1)\n",
    "#         x=x.transpose(1,2)\n",
    "#         x=x.view(-1,num_channels)\n",
    "#         x1=self.layer1(x)\n",
    "#         output=F.relu(x1)\n",
    "#         output=output.view(batch_size,height,1)\n",
    "\n",
    "#         return output.squeeze(1)\n",
    "\n",
    "class ModifiedAFFWithSoftmax(nn.Module):\n",
    "    def __init__(self, channels=5, r=4, num_features=5):\n",
    "        super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        inter_channels = int(channels // r )\n",
    "        \n",
    "        \n",
    "        self.local_att = nn.Sequential(\n",
    "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "        \n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "        self.mlp=nn.Linear(2,1)\n",
    "    def forward(self, x):\n",
    "        xl = self.local_att(x)\n",
    "        xg = self.global_att(x)\n",
    "        repeat_num=xl.size(2)\n",
    "        xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "        xlg = torch.cat((xl, xg), dim=3)\n",
    "        batch_size, num_channels, height, width = xlg.size()\n",
    "        xlg = xlg.view(-1, width)\n",
    "        xlg = F.relu(self.mlp(xlg))\n",
    "        xlg=xlg.view(\n",
    "        batch_size,num_channels,height,-1)\n",
    "        # xlg = xl + xg\n",
    "        # xlg = xlg - xl\n",
    "        \n",
    "        weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = weights.view_as(xlg)\n",
    "        \n",
    "        separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "        weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "        output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "        return output.squeeze(1)\n",
    "# 初始化模型\n",
    "model = ModifiedAFFWithSoftmax()\n",
    "\n",
    "# 生成模拟输入数据\n",
    "train_tensor = torch.from_numpy(train_data)  # Batch size 8784, 18 features, 41 sites, width 1\n",
    "train_tensor = train_tensor.float()\n",
    "\n",
    "# 前向传播\n",
    "output_tensor = model(train_tensor)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1635],\n",
       "         [0.1896],\n",
       "         [0.1889],\n",
       "         ...,\n",
       "         [0.2917],\n",
       "         [0.1759],\n",
       "         [0.1730]],\n",
       "\n",
       "        [[0.1640],\n",
       "         [0.1896],\n",
       "         [0.1883],\n",
       "         ...,\n",
       "         [0.2922],\n",
       "         [0.1759],\n",
       "         [0.1730]],\n",
       "\n",
       "        [[0.1640],\n",
       "         [0.1896],\n",
       "         [0.1889],\n",
       "         ...,\n",
       "         [0.2917],\n",
       "         [0.1754],\n",
       "         [0.1730]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.1642],\n",
       "         [0.1833],\n",
       "         [0.2078],\n",
       "         ...,\n",
       "         [0.4553],\n",
       "         [0.2065],\n",
       "         [0.3088]],\n",
       "\n",
       "        [[0.1642],\n",
       "         [0.1828],\n",
       "         [0.2073],\n",
       "         ...,\n",
       "         [0.4544],\n",
       "         [0.2061],\n",
       "         [0.3165]],\n",
       "\n",
       "        [[0.1642],\n",
       "         [0.1828],\n",
       "         [0.2086],\n",
       "         ...,\n",
       "         [0.4548],\n",
       "         [0.2075],\n",
       "         [0.3089]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.02695417789757413],\n",
       "         [0.0646900269541779],\n",
       "         [0.005390835579514829],\n",
       "         ...,\n",
       "         [0.024258760107816715],\n",
       "         [0.11051212938005392],\n",
       "         [0.013477088948787075]],\n",
       "\n",
       "        [[0.042372881355932215],\n",
       "         [0.10734463276836156],\n",
       "         [0.06779661016949153],\n",
       "         ...,\n",
       "         [0.03954802259887007],\n",
       "         [0.09039548022598871],\n",
       "         [0.011299435028248596]],\n",
       "\n",
       "        [[0.08808290155440412],\n",
       "         [0.11658031088082903],\n",
       "         [0.028497409326424878],\n",
       "         ...,\n",
       "         [0.17098445595854922],\n",
       "         [0.04663212435233159],\n",
       "         [0.0829015544041451]],\n",
       "\n",
       "        [[0.14138817480719792],\n",
       "         [0.08997429305912596],\n",
       "         [0.06426735218508996],\n",
       "         ...,\n",
       "         [0.7609254498714653],\n",
       "         [0.09254498714652953],\n",
       "         [0.22107969151670953]],\n",
       "\n",
       "        [[0.5424836601307189],\n",
       "         [0.5947712418300654],\n",
       "         [0.522875816993464],\n",
       "         ...,\n",
       "         [0.5032679738562092],\n",
       "         [0.5686274509803922],\n",
       "         [0.5620915032679739]]],\n",
       "\n",
       "\n",
       "       [[[0.02695417789757413],\n",
       "         [0.0646900269541779],\n",
       "         [0.005390835579514829],\n",
       "         ...,\n",
       "         [0.02695417789757413],\n",
       "         [0.11051212938005392],\n",
       "         [0.013477088948787075]],\n",
       "\n",
       "        [[0.042372881355932215],\n",
       "         [0.10734463276836156],\n",
       "         [0.06779661016949153],\n",
       "         ...,\n",
       "         [0.03954802259887007],\n",
       "         [0.09039548022598871],\n",
       "         [0.011299435028248596]],\n",
       "\n",
       "        [[0.08808290155440412],\n",
       "         [0.11658031088082903],\n",
       "         [0.028497409326424878],\n",
       "         ...,\n",
       "         [0.17098445595854922],\n",
       "         [0.04663212435233159],\n",
       "         [0.0829015544041451]],\n",
       "\n",
       "        [[0.14395886889460152],\n",
       "         [0.08997429305912596],\n",
       "         [0.06426735218508996],\n",
       "         ...,\n",
       "         [0.7609254498714653],\n",
       "         [0.09254498714652953],\n",
       "         [0.22107969151670953]],\n",
       "\n",
       "        [[0.5424836601307189],\n",
       "         [0.5947712418300654],\n",
       "         [0.522875816993464],\n",
       "         ...,\n",
       "         [0.5032679738562092],\n",
       "         [0.5686274509803922],\n",
       "         [0.5620915032679739]]],\n",
       "\n",
       "\n",
       "       [[[0.02695417789757413],\n",
       "         [0.0646900269541779],\n",
       "         [0.005390835579514829],\n",
       "         ...,\n",
       "         [0.024258760107816715],\n",
       "         [0.1078167115902965],\n",
       "         [0.013477088948787075]],\n",
       "\n",
       "        [[0.042372881355932215],\n",
       "         [0.10734463276836156],\n",
       "         [0.06779661016949153],\n",
       "         ...,\n",
       "         [0.03954802259887007],\n",
       "         [0.09039548022598871],\n",
       "         [0.011299435028248596]],\n",
       "\n",
       "        [[0.08808290155440412],\n",
       "         [0.11658031088082903],\n",
       "         [0.028497409326424878],\n",
       "         ...,\n",
       "         [0.17098445595854922],\n",
       "         [0.04663212435233159],\n",
       "         [0.0829015544041451]],\n",
       "\n",
       "        [[0.14395886889460152],\n",
       "         [0.08997429305912596],\n",
       "         [0.06683804627249355],\n",
       "         ...,\n",
       "         [0.7609254498714653],\n",
       "         [0.09254498714652953],\n",
       "         [0.22107969151670953]],\n",
       "\n",
       "        [[0.5424836601307189],\n",
       "         [0.5947712418300654],\n",
       "         [0.522875816993464],\n",
       "         ...,\n",
       "         [0.5032679738562092],\n",
       "         [0.5686274509803922],\n",
       "         [0.5620915032679739]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.04582210242587603],\n",
       "         [0.21563342318059303],\n",
       "         [0.0835579514824798],\n",
       "         ...,\n",
       "         [0.13477088948787064],\n",
       "         [0.2075471698113208],\n",
       "         [0.12938005390835583]],\n",
       "\n",
       "        [[0.1638418079096045],\n",
       "         [0.22033898305084748],\n",
       "         [0.2514124293785311],\n",
       "         ...,\n",
       "         [0.2655367231638418],\n",
       "         [0.18079096045197737],\n",
       "         [0.1384180790960452]],\n",
       "\n",
       "        [[0.1761658031088083],\n",
       "         [0.2046632124352332],\n",
       "         [0.1917098445595855],\n",
       "         ...,\n",
       "         [0.8471502590673575],\n",
       "         [0.1968911917098446],\n",
       "         [0.25647668393782386]],\n",
       "\n",
       "        [[0.25964010282776345],\n",
       "         [0.04627249357326479],\n",
       "         [0.699228791773779],\n",
       "         ...,\n",
       "         [0.961439588688946],\n",
       "         [0.21850899742930593],\n",
       "         [0.8277634961439588]],\n",
       "\n",
       "        [[0.2091503267973856],\n",
       "         [0.23529411764705882],\n",
       "         [0.22875816993464054],\n",
       "         ...,\n",
       "         [0.24836601307189538],\n",
       "         [0.23529411764705882],\n",
       "         [0.2745098039215686]]],\n",
       "\n",
       "\n",
       "       [[[0.04582210242587603],\n",
       "         [0.21563342318059303],\n",
       "         [0.0808625336927224],\n",
       "         ...,\n",
       "         [0.13477088948787064],\n",
       "         [0.2075471698113208],\n",
       "         [0.13477088948787064]],\n",
       "\n",
       "        [[0.1638418079096045],\n",
       "         [0.22033898305084748],\n",
       "         [0.2514124293785311],\n",
       "         ...,\n",
       "         [0.2655367231638418],\n",
       "         [0.18079096045197737],\n",
       "         [0.14689265536723162]],\n",
       "\n",
       "        [[0.1761658031088083],\n",
       "         [0.20207253886010365],\n",
       "         [0.18911917098445596],\n",
       "         ...,\n",
       "         [0.8471502590673575],\n",
       "         [0.1968911917098446],\n",
       "         [0.26424870466321243]],\n",
       "\n",
       "        [[0.25964010282776345],\n",
       "         [0.04627249357326479],\n",
       "         [0.699228791773779],\n",
       "         ...,\n",
       "         [0.961439588688946],\n",
       "         [0.2159383033419023],\n",
       "         [0.8483290488431877]],\n",
       "\n",
       "        [[0.2091503267973856],\n",
       "         [0.23529411764705882],\n",
       "         [0.22875816993464054],\n",
       "         ...,\n",
       "         [0.24836601307189538],\n",
       "         [0.23529411764705882],\n",
       "         [0.2745098039215686]]],\n",
       "\n",
       "\n",
       "       [[[0.04582210242587603],\n",
       "         [0.21563342318059303],\n",
       "         [0.0808625336927224],\n",
       "         ...,\n",
       "         [0.13477088948787064],\n",
       "         [0.2102425876010782],\n",
       "         [0.12938005390835583]],\n",
       "\n",
       "        [[0.1638418079096045],\n",
       "         [0.22033898305084748],\n",
       "         [0.2514124293785311],\n",
       "         ...,\n",
       "         [0.2655367231638418],\n",
       "         [0.18079096045197737],\n",
       "         [0.1384180790960452]],\n",
       "\n",
       "        [[0.1761658031088083],\n",
       "         [0.20207253886010365],\n",
       "         [0.18911917098445596],\n",
       "         ...,\n",
       "         [0.8471502590673575],\n",
       "         [0.19430051813471505],\n",
       "         [0.25647668393782386]],\n",
       "\n",
       "        [[0.25964010282776345],\n",
       "         [0.04627249357326479],\n",
       "         [0.6966580976863753],\n",
       "         ...,\n",
       "         [0.961439588688946],\n",
       "         [0.2159383033419023],\n",
       "         [0.8277634961439588]],\n",
       "\n",
       "        [[0.2091503267973856],\n",
       "         [0.23529411764705882],\n",
       "         [0.23529411764705882],\n",
       "         ...,\n",
       "         [0.24836601307189538],\n",
       "         [0.2418300653594771],\n",
       "         [0.2745098039215686]]]], dtype=object)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
