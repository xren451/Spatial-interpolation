{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geographiclib.geodesic import Geodesic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(random.randint(0, 1))\n",
    "    return array\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "random_array = generate_random_array(10)\n",
    "print(random_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取原来的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_col.pkl', 'rb') as file:\n",
    "    train_ori = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_col.pkl', 'rb') as file:\n",
    "    test_ori = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=np.load('data/all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 103)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=14\n",
    "train_data=X_raw[:,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "missing_columns = np.where(columns_with_missing_data)[0]\n",
    "result = np.delete(train_data, missing_columns, axis=1)\n",
    "result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  15,  28,  34,  45,  47,  55,  60,  82,  83,  84,  85,  86,\n",
       "        87,  88,  89,  90,  91,  92,  93,  94,  96,  98,  99, 100, 102],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 77)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result=train_data\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 1)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori[\"train_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 1)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ori[\"test_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = result.mean(axis=0)\n",
    "# std = result.std(axis=0)\n",
    "# result = (result - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70]\n",
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]\n",
    "test_list=[52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76]\n",
    "# test_list=[71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=result[:,train_list]\n",
    "test_data=result[:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.transpose(train_data, (0, 2, 1))\n",
    "# test_data = np.transpose(test_data, (0, 2, 1))\n",
    "# train_data = np.reshape(train_data, (8784, 63,1))\n",
    "# test_data = np.reshape(test_data, (8784, 40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 1)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 1)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 103)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = np.delete(train_data, missing_columns, axis=1)\n",
    "# X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=5\n",
    "# train_data=X_raw[:,j,:]\n",
    "train_fusion_data=np.delete(X_raw, missing_columns, axis=2)\n",
    "# train_fusion_data[train_fusion_data == 0] = 0.00001\n",
    "# train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)\n",
    "# train_fusion_data=X_raw\n",
    "# train_data = np.nan_to_num(train_data, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fusion_data[train_fusion_data == 0] = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 77)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 14)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori[\"fusion_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 14)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ori[\"fusion_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 14, 77)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_remove = [11, 15, 16 ,17]\n",
    "train_fusion_data = np.delete(train_fusion_data, columns_to_remove, axis=1)\n",
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 17568\n",
      "Number of zeros in the 6th feature: 254736\n",
      "Number of zeros in the 6th feature: 254736\n",
      "Number of zeros in the 6th feature: 509472\n",
      "Number of zeros in the 6th feature: 26352\n",
      "Number of zeros in the 6th feature: 17568\n",
      "Number of zeros in the 6th feature: 755424\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5,train_fusion_data.shape[1]):\n",
    "#     nan_count = np.sum(np.isnan(train_fusion_data[:,i,:]))\n",
    "#     print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_index in range(5, train_fusion_data.shape[1]):\n",
    "    feature_column = train_fusion_data[:, feature_index, :]\n",
    "    min=np.nanmin(feature_column)\n",
    "    max=np.nanmax(feature_column)\n",
    "    norm_data=(feature_column - min) / (max - min)\n",
    "    train_fusion_data[:, feature_index, :]=norm_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data[train_fusion_data == 0] = 0.00001\n",
    "train_fusion_data=np.nan_to_num(train_fusion_data, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n",
      "Number of zeros in the 6th feature: 0\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5,train_fusion_data.shape[1]):\n",
    "#     nan_count = np.sum(np.isnan(train_fusion_data[:,i,:]))\n",
    "#     print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the 6th feature: 254736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # train_fusion_data[:,5,:]\n",
    "# nan_count = np.sum(np.isnan(norm_data))\n",
    "# print(f\"Number of zeros in the 6th feature: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "# missing_columns = np.where(columns_with_missing_data)[0]\n",
    "# result = np.delete(train_data, missing_columns, axis=2)\n",
    "# # result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_arr = np.unique(missing_columns)\n",
    "# unique_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.05000697e-01, 2.17916734e-01, 2.17916734e-01, ...,\n",
       "         2.17916734e-01, 2.19241456e-01, 2.18910276e-01],\n",
       "        [8.24267782e-01, 7.85216179e-01, 8.53556485e-01, ...,\n",
       "         8.84239888e-01, 8.67503487e-01, 8.82845188e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         6.87315634e-01, 6.72566372e-01, 7.22713864e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.06656599e-01, 2.19241456e-01, 2.18910276e-01, ...,\n",
       "         2.19241456e-01, 2.20234997e-01, 2.19903817e-01],\n",
       "        [8.20083682e-01, 7.88005579e-01, 8.56345886e-01, ...,\n",
       "         8.84239888e-01, 8.66108787e-01, 8.81450488e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         6.84365782e-01, 6.72566372e-01, 7.19764012e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.08974862e-01, 2.19903817e-01, 2.18910276e-01, ...,\n",
       "         2.21228539e-01, 2.21890900e-01, 2.21559719e-01],\n",
       "        [8.15899582e-01, 7.86610879e-01, 8.59135286e-01, ...,\n",
       "         8.84239888e-01, 8.67503487e-01, 8.82845188e-01],\n",
       "        [6.43067847e-01, 4.89675516e-01, 6.87315634e-01, ...,\n",
       "         6.84365782e-01, 6.72566372e-01, 7.19764012e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.23546802e-01, 2.29176870e-01, 2.25865065e-01, ...,\n",
       "         2.21228539e-01, 2.19572637e-01, 2.19241456e-01],\n",
       "        [7.61506276e-01, 7.28033473e-01, 8.29846583e-01, ...,\n",
       "         8.52161785e-01, 8.49372385e-01, 8.60529986e-01],\n",
       "        [6.22418879e-01, 4.18879056e-01, 6.81415929e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]],\n",
       "\n",
       "       [[2.01200000e+03, 2.01200000e+03, 2.01200000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.24871524e-01, 2.28514509e-01, 2.26527426e-01, ...,\n",
       "         2.20897358e-01, 2.20234997e-01, 2.20566178e-01],\n",
       "        [7.67085077e-01, 7.25244073e-01, 8.32635983e-01, ...,\n",
       "         8.52161785e-01, 8.49372385e-01, 8.59135286e-01],\n",
       "        [6.22418879e-01, 4.18879056e-01, 6.81415929e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]],\n",
       "\n",
       "       [[2.01100000e+03, 2.01100000e+03, 2.01100000e+03, ...,\n",
       "         2.01200000e+03, 2.01200000e+03, 2.01200000e+03],\n",
       "        [1.20000000e+01, 1.20000000e+01, 1.20000000e+01, ...,\n",
       "         1.20000000e+01, 1.20000000e+01, 1.20000000e+01],\n",
       "        [3.10000000e+01, 3.10000000e+01, 3.10000000e+01, ...,\n",
       "         3.10000000e+01, 3.10000000e+01, 3.10000000e+01],\n",
       "        ...,\n",
       "        [2.02020073e-01, 2.15598471e-01, 2.16592013e-01, ...,\n",
       "         2.19241456e-01, 2.18910276e-01, 2.18579095e-01],\n",
       "        [8.27057183e-01, 7.88005579e-01, 8.50767085e-01, ...,\n",
       "         8.53556485e-01, 8.50767085e-01, 8.57740586e-01],\n",
       "        [6.43067847e-01, 4.86725664e-01, 6.87315634e-01, ...,\n",
       "         7.10914454e-01, 6.13569322e-01, 7.25663717e-01]]])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=train_fusion_data\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 14, 77)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data shape: (8784, 14, 93)\n"
     ]
    }
   ],
   "source": [
    "# normalized_data = np.zeros_like(result)\n",
    "# for feature_index in range(result.shape[1]):  # 遍历每个特征\n",
    "#     feature_values = result[:, feature_index, :]  # 当前特征的所有数据点\n",
    "#     non_zero_mask = feature_values != 0  # 非零值掩码\n",
    "#     non_zero_values = feature_values[non_zero_mask]  # 提取非零值\n",
    "\n",
    "#     # 计算非零值的均值和标准差\n",
    "#     mean = np.mean(non_zero_values)\n",
    "#     std = np.std(non_zero_values)\n",
    "\n",
    "#     # 只在非零值上应用 Z-score 标准化\n",
    "#     if std > 0:\n",
    "#         normalized_values = (non_zero_values - mean) / std\n",
    "#         feature_values[non_zero_mask] = normalized_values  # 将计算结果赋值回相应位置\n",
    "#     else:\n",
    "#         # 如果所有非零值都相等（std=0），可以选择保留原值或其他处理\n",
    "#         feature_values[non_zero_mask] = non_zero_values  # 这里选择保留原值\n",
    "\n",
    "#     normalized_data[:, feature_index, :] = feature_values  # 更新标准化数据\n",
    "# print(\"Normalized data shape:\", normalized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70]\n",
    "train_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]\n",
    "test_list=[52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76]\n",
    "# test_list=[71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data=result[:,:,train_list]\n",
    "test_fusion_data=result[:,:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_data = np.transpose(train_fusion_data, (0, 2, 1))\n",
    "test_fusion_data = np.transpose(test_fusion_data, (0, 2, 1))\n",
    "# train_fusion_data = np.reshape(train_fusion_data, (8784, 63,18))\n",
    "# test_fusion_data = np.reshape(test_fusion_data, (8784, 40,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 14)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 14)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fusion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.reshape(train_data, (8784, 41,18))\n",
    "# test_data = np.reshape(test_data, (8784, 62,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_list=[0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=X_raw[:,timestap_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap.astype(np.int32)\n",
    "timestap=timestap.astype(np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timestap)):\n",
    "    for m in range(1,4):\n",
    "        timestap[i,m,1]=timestap[i,m,1].zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap=timestap[:,:,1]\n",
    "timestap_result=[]\n",
    "for i in range(len(timestap)):\n",
    "    x=timestap[i,0]+timestap[i,1]+timestap[i,2]+'-'+timestap[i,3]\n",
    "    timestap_result.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_result=np.array(timestap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784,)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestap_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invid_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(False)\n",
    "    return np.array(array)\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "train_invid_mask=[]\n",
    "test_invid_mask=[]\n",
    "for i in range(len(timestap)):\n",
    "    random_array = generate1_random_array(train_data.shape[1])\n",
    "    random_array1 = generate1_random_array(test_data.shape[1])\n",
    "    train_invid_mask.append(random_array)\n",
    "    test_invid_mask.append(random_array1)\n",
    "train_invid_mask=np.array(train_invid_mask)\n",
    "test_invid_mask=np.array(test_invid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_invid_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "            array.append(random.choice([False,True]))\n",
    "    return np.array(array)\n",
    "\n",
    "# 生成长度为10的随机数组\n",
    "test_mask=[]\n",
    "random_array=generate_random_array(test_data.shape[1])\n",
    "for i in range(len(timestap)):\n",
    "    test_mask.append(random_array)\n",
    "test_mask=np.array(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 地理位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建r_pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(\"D:\\Spatial_interpolation\\SSIN\\data\\Station_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.drop([  4,  15,  28,  34,  45,  47,  55,  60,  82,  83,  84,  85,  86,\n",
    "        87,  88,  89,  90,  91,  92,  93,  94,  96,  98,  99, 100, 102],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 8)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 52, 14)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori[\"fusion_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 25, 14)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ori[\"fusion_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=stations.iloc[0:52]\n",
    "df_test=stations.iloc[52:77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 8)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 8)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_angle_mat(df, out_path):\n",
    "    lons, lats = df[\"lon\"].values, df[\"lat\"].values\n",
    "    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n",
    "\n",
    "    for i in range(len(lons)):\n",
    "        for j in range(len(lons)):\n",
    "            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n",
    "            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n",
    "            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n",
    "\n",
    "    print(dist_angle_mat.shape)\n",
    "    # print(dist_angle_mat)\n",
    "    np.save(out_path, dist_angle_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 52, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_train.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_train, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 25, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}/BW_132_data\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_test.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_test, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_from_train_data(train_info_df, r_pos_mat):\n",
    "    # info_df = pd.read_csv(info_path)\n",
    "    # ori_r_pos_mat = np.load(relative_pos_mat_path)\n",
    "\n",
    "    # is_test = info_df[\"is_test\"].values\n",
    "    # train_mask = np.where(is_test == 0, True, False)\n",
    "    # train_info_df = info_df.loc[train_mask, :]\n",
    "\n",
    "    lat_mean, lat_std, lat_max, lat_min = train_info_df[\"lat\"].mean(), train_info_df[\"lat\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lat\"].max(), train_info_df[\"lat\"].min()\n",
    "    lon_mean, lon_std, lon_max, lon_min = train_info_df[\"lon\"].mean(), train_info_df[\"lon\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lon\"].max(), train_info_df[\"lon\"].min()\n",
    "\n",
    "    # indexes = np.where(train_mask)[0]\n",
    "    # idx_i, idx_j = np.ix_(indexes, indexes)\n",
    "    # r_pos_mat = ori_r_pos_mat[idx_i, idx_j, :]\n",
    "\n",
    "    r_dist_mat = r_pos_mat[:, :, 0]\n",
    "    r_angle_mat = r_pos_mat[:, :, 1]\n",
    "\n",
    "    r_dist_mean, r_dist_std, r_dist_max, r_dist_min = np.mean(r_dist_mat), np.std(r_dist_mat), \\\n",
    "                                                      np.max(r_dist_mat), np.min(r_dist_mat),\n",
    "    r_angle_mean, r_angle_std, r_angle_max, r_angle_min = np.mean(r_angle_mat), np.std(r_angle_mat), \\\n",
    "                                                          np.max(r_angle_mat), np.min(r_angle_mat)\n",
    "\n",
    "    stat_dict = {}\n",
    "    stat_dict[\"lat_mean\"], stat_dict[\"lat_std\"], stat_dict[\"lat_max\"], stat_dict[\"lat_min\"] = \\\n",
    "        lat_mean, lat_std, lat_max, lat_min\n",
    "    stat_dict[\"lon_mean\"], stat_dict[\"lon_std\"], stat_dict[\"lon_max\"], stat_dict[\"lon_min\"] = \\\n",
    "        lon_mean, lon_std, lon_max, lon_min\n",
    "\n",
    "    stat_dict[\"r_dist_mean\"], stat_dict[\"r_dist_std\"], stat_dict[\"r_dist_max\"], stat_dict[\"r_dist_min\"] = \\\n",
    "        r_dist_mean, r_dist_std, r_dist_max, r_dist_min\n",
    "    stat_dict[\"r_angle_mean\"], stat_dict[\"r_angle_std\"], stat_dict[\"r_angle_max\"], stat_dict[\"r_angle_min\"] = \\\n",
    "        r_angle_mean, r_angle_std, r_angle_max, r_angle_min\n",
    "\n",
    "    print(\"Calculates the statistics of training data. Done!\")\n",
    "\n",
    "    # with open(\"./data/hk_data_stats.pkl\".format(out_name), \"wb\") as fp:\n",
    "    #     pickle.dump(stat_dict, fp)\n",
    "\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_train.npy\")\n",
    "test_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculates the statistics of training data. Done!\n"
     ]
    }
   ],
   "source": [
    "stat_dict=generate_stat_from_train_data(df_train, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df[:, :, 0] = (train_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "train_df[:, :, 1] = (train_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[:, :, 0] = (test_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "test_df[:, :, 1] = (test_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # 将经纬度从度转换为弧度\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine 公式\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    \n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c  # 地球半径的平均半径，单位公里\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(latitudes, longitudes):\n",
    "    num_sites = len(latitudes)\n",
    "    distance_matrix = np.zeros((num_sites, num_sites))\n",
    "    \n",
    "    for i in range(num_sites):\n",
    "        for j in range(i + 1, num_sites):\n",
    "            dist = haversine(latitudes[i], longitudes[i], latitudes[j], longitudes[j])\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist  # 因为距离是对称的\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index_from_nearest_neighbors(distance_matrix, num_neighbors=3):\n",
    "    num_sites = distance_matrix.shape[0]\n",
    "    edge_index = []\n",
    "\n",
    "    for i in range(num_sites):\n",
    "        # 获取与站点 i 距离最近的 num_neighbors 个站点的索引\n",
    "        neighbors_indices = np.argsort(distance_matrix[i])[:num_neighbors + 1]  # +1 因为最近的包括自己\n",
    "        for j in neighbors_indices:\n",
    "            if i != j:  # 避免自连接\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "    return torch.tensor(edge_index).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_train,latitudes_train=df_train[\"lon\"].values, df_train[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_train = compute_distance_matrix(longitudes_train, latitudes_train)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_train = create_edge_index_from_nearest_neighbors(distance_matrix_train, num_neighbors=3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudes_test,latitudes_test=df_test[\"lon\"].values, df_test[\"lat\"].values\n",
    "# 计算距离矩阵\n",
    "distance_matrix_test = compute_distance_matrix(longitudes_test, latitudes_test)\n",
    "\n",
    "# 生成边索引\n",
    "edge_index_test = create_edge_index_from_nearest_neighbors(distance_matrix_test, num_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 156])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建最后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori[\"fusion_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ori[\"fusion_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "test={\"test_data\":test_ori[\"test_data\"],\"fusion_data\":test_ori[\"fusion_data\"],\"invalid_masks\":test_ori[\"invalid_masks\"],\"test_masks\":test_ori[\"test_masks\"],\"r_pos_mat\":test_ori[\"r_pos_mat\"],\"timestamps\":test_ori[\"timestamps\"],\"edg_index\":edge_index_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "train={\"train_data\":train_ori[\"train_data\"],\"fusion_data\":train_ori[\"fusion_data\"],\"invalid_masks\":train_ori[\"invalid_masks\"],\"r_pos_mat\":train_ori[\"r_pos_mat\"],\"timestamps\":train_ori[\"timestamps\"],\"edg_index\":edge_index_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_data', 'fusion_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps', 'edg_index']\n"
     ]
    }
   ],
   "source": [
    "print(list(test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_data', 'fusion_data', 'invalid_masks', 'r_pos_mat', 'timestamps', 'edg_index']\n"
     ]
    }
   ],
   "source": [
    "print(list(train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_pickle(\"2012-2014_data_train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_bw_train = pd.read_pickle(\"2012-2014_data_bw_train.pkl\")\n",
    "# data_bw_test = pd.read_pickle(\"2012-2014_data_bw_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_data', 'invalid_masks', 'test_masks', 'r_pos_mat', 'timestamps'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_bw_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 132, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_bw_train[\"r_pos_mat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 41)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[\"invalid_masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,5:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 9, 71, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 213])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edg_index_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8784, 71, 1])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "train=data[\"fusion_data\"]\n",
    "train[0]\n",
    "train=data[\"fusion_data\"]\n",
    "train_data_expanded = np.expand_dims(train, axis=-1)\n",
    "train_data_expanded.shape\n",
    "train_data_expanded = np.transpose(train_data_expanded, (0, 2, 1,3))\n",
    "train_data_expanded.shape\n",
    "train_data=train_data_expanded[:,5:,:,:]\n",
    "edg_index_in=data[\"edg_index\"]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class ModifiedAFFWithSoftmax(nn.Module):\n",
    "    def __init__(self, num_timestamps=8784, num_features=9, r=4):\n",
    "        super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "        self.num_timestamps = num_timestamps\n",
    "        self.num_features = num_features\n",
    "        # self.num_sites = num_sites\n",
    "        inter_channels = int(num_features // r * num_timestamps)\n",
    "        \n",
    "        # 图卷积网络\n",
    "\n",
    "        self.GCN1=GCNConv(num_timestamps * num_features, inter_channels)\n",
    "        # self.batch1=nn.BatchNorm1d(inter_channels)\n",
    "        self.relu1=nn.ReLU(inplace=True)\n",
    "        self.GCN2=GCNConv(inter_channels, num_timestamps * num_features)\n",
    "        # self.batch2=nn.BatchNorm1d(num_timestamps * num_features)\n",
    "        self.relu2=nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 传统卷积部分调整为一维操作，以处理1D特征数据\n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_features, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, num_features, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features),   \n",
    "        )\n",
    "\n",
    "        self.mlp=nn.Linear(2,1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 调整x的形状以适应图卷积网络的输入\n",
    "        num_sites=x.size(2)\n",
    "        x_gcn=x\n",
    "        x_gcn=x_gcn.squeeze(-1).permute(2, 1, 0)\n",
    "        x_gcn = x_gcn.reshape(num_sites, -1)  # (103, 8784*9)\n",
    "\n",
    "\n",
    "        # GCN 处理\n",
    "        x_gcn=self.GCN1(x_gcn, edge_index)\n",
    "        # x_gcn=self.batch1(x_gcn)\n",
    "        x_gcn=self.relu1(x_gcn)\n",
    "        x_gcn=self.GCN2(x_gcn, edge_index)\n",
    "        # xl=self.batch2(x_gcn)\n",
    "        xl=self.relu2(x_gcn)\n",
    "        \n",
    "        # 调整为原始时间戳维度\n",
    "        xl = xl.view(num_sites,  self.num_features,self.num_timestamps).permute(2, 1, 0).unsqueeze(-1)  # (8784, 9, 103, 1)\n",
    "\n",
    "        # Global attention 处理\n",
    "        xg = self.global_att(x)\n",
    "        repeat_num=xl.size(2)\n",
    "        xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "        xlg = torch.cat((xl, xg), dim=3)\n",
    "        batch_size, num_channels, height, width = xlg.size()\n",
    "        xlg = xlg.view(-1, width)\n",
    "        xlg = F.relu(self.mlp(xlg))\n",
    "        xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "\n",
    "        weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = weights.view_as(xlg)\n",
    "        \n",
    "        separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "        weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "        output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "        return output.squeeze(1)\n",
    "# class ModifiedAFFWithSoftmax(nn.Module):\n",
    "#     def __init__(self, channels=9, r=4, num_features=9):\n",
    "#         super(ModifiedAFFWithSoftmax, self).__init__()\n",
    "#         self.num_features = num_features\n",
    "#         inter_channels = int(channels // r )\n",
    "        \n",
    "        \n",
    "#         self.local_att = nn.Sequential(\n",
    "#             nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(inter_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(channels),\n",
    "#         )\n",
    "        \n",
    "#         self.global_att = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(inter_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(channels),   \n",
    "#         )\n",
    "\n",
    "#         self.mlp=nn.Linear(2,1)\n",
    "#     def forward(self, x):\n",
    "#         xl = self.local_att(x)\n",
    "#         xg = self.global_att(x)\n",
    "#         repeat_num=xl.size(2)\n",
    "#         xg=xg.repeat(1, 1, repeat_num, 1)\n",
    "#         xlg = torch.cat((xl, xg), dim=3)\n",
    "#         batch_size, num_channels, height, width = xlg.size()\n",
    "#         xlg = xlg.view(-1, width)\n",
    "#         xlg = F.relu(self.mlp(xlg))\n",
    "#         xlg=xlg.view(batch_size,num_channels,height,-1)\n",
    "#         # xlg = xl + xg\n",
    "#         # xlg = xlg - xl\n",
    "        \n",
    "#         weights = xlg.view(xlg.size(0), self.num_features, -1)\n",
    "#         weights = F.softmax(weights, dim=1)\n",
    "#         weights = weights.view_as(xlg)\n",
    "        \n",
    "#         separated_weights = torch.chunk(weights, self.num_features, dim=1)\n",
    "#         weighted_features = [x[:, i:i+1, :, :] * separated_weights[i] for i in range(self.num_features)]\n",
    "        \n",
    "#         output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)\n",
    "\n",
    "#         return output.squeeze(1)\n",
    "# 初始化模型\n",
    "model = ModifiedAFFWithSoftmax()\n",
    "\n",
    "# 生成模拟输入数据\n",
    "train_tensor = torch.from_numpy(train_data)  # Batch size 8784, 18 features, 41 sites, width 1\n",
    "train_tensor = train_tensor.float()\n",
    "\n",
    "# 前向传播\n",
    "output_tensor = model(train_tensor,edg_index_in)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
