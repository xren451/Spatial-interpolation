{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geographiclib.geodesic import Geodesic\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filenames(folder_path):\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        filenames.append(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:\\Spatial_interpolation\\SSIN\\data\\india\"\n",
    "all_filenames = get_all_filenames(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8784, 18, 103)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw=np.load('data/all.npy')\n",
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_result=[]\n",
    "for filename in all_filenames:\n",
    "    x=pd.read_csv(folder_path+\"/\"+filename)\n",
    "    x=np.array(x)\n",
    "    x=np.reshape(x,(105120, 4 ,1))\n",
    "    file_result.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=file_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(file_result)):\n",
    "    result = np.append(result,file_result[i] , axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105120, 4, 8)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=3\n",
    "train_data=X_raw[:,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_data = np.any(np.isnan(train_data), axis=0)\n",
    "missing_columns = np.where(columns_with_missing_data)[0]\n",
    "result = np.delete(train_data, missing_columns, axis=1)\n",
    "result = (result - np.min(result)) / (np.max(result) - np.min(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105120, 8)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=[0,1,2,3]\n",
    "test_list=[4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=result[:,train_list]\n",
    "test_data=result[:,test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.transpose(train_data, (0, 2, 1))\n",
    "test_data = np.transpose(test_data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.reshape(train_data, (105120, 4,1))\n",
    "# test_data = np.reshape(test_data, (105120, 4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "delet_list=[]\n",
    "for i in range(len(train_data)):\n",
    "    temp=np.reshape(train_data[i],(4))\n",
    "    if (len(set(temp)) == 1):\n",
    "        delet_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.delete(train_data,delet_list,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timestaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=['2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023']\n",
    "months=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "days=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31']\n",
    "hours=['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestaps=[]\n",
    "for year in years:\n",
    "    for month in  months:\n",
    "        for day in days:\n",
    "            for hour in hours:\n",
    "                x=str(year)+str(month)+str(day)+\"-\"+str(hour)\n",
    "                timestaps.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestaps_result=timestaps[:105120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestaps_train_result=timestaps[:len(train_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestap_result=np.array(timestaps_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestaps_train_result=np.array(timestaps_train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105120,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestap_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104823,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestaps_train_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invid_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "        array.append(False)\n",
    "    return np.array(array)\n",
    " \n",
    "# 生成长度为10的随机数组\n",
    "train_invid_mask=[]\n",
    "test_invid_mask=[]\n",
    "for i in range(len(timestap_result)):\n",
    "    # random_array = generate1_random_array(len(train_data[0]))\n",
    "    random_array1 = generate1_random_array(len(test_data[0]))\n",
    "    # train_invid_mask.append(random_array)\n",
    "    test_invid_mask.append(random_array1)\n",
    "\n",
    "for i in range(len(timestaps_train_result)):\n",
    "    random_array = generate1_random_array(len(train_data[0]))\n",
    "    # random_array1 = generate1_random_array(len(test_data[0]))\n",
    "    train_invid_mask.append(random_array)\n",
    "    # test_invid_mask.append(random_array1)\n",
    "    \n",
    "train_invid_mask=np.array(train_invid_mask)\n",
    "test_invid_mask=np.array(test_invid_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_mask掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "def generate_random_array(length):\n",
    "    array = []\n",
    "    for _ in range(length):\n",
    "            array.append(random.choice([False,True]))\n",
    "    return np.array(array)\n",
    "\n",
    "# 生成长度为10的随机数组\n",
    "test_mask=[]\n",
    "random_array=generate_random_array(test_data.shape[1])\n",
    "for i in range(len(timestap)):\n",
    "    test_mask.append(random_array)\n",
    "test_mask=np.array(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 地理位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建r_pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(\"D:\\Spatial_interpolation\\SSIN\\data\\Locations_of_India.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=stations.iloc[0:4]\n",
    "df_test=stations.iloc[4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_angle_mat(df, out_path):\n",
    "    lons, lats = df[\"lon\"].values, df[\"lat\"].values\n",
    "    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n",
    "\n",
    "    for i in range(len(lons)):\n",
    "        for j in range(len(lons)):\n",
    "            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n",
    "            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n",
    "            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n",
    "\n",
    "    print(dist_angle_mat.shape)\n",
    "    # print(dist_angle_mat)\n",
    "    np.save(out_path, dist_angle_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_train.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_train, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../data\"\n",
    "\n",
    "    # HK dataset\n",
    "    # info_path = f\"{base_dir}/HK_123_data/hko_stations_info.csv\"\n",
    "    out_dir = f\"{base_dir}\"\n",
    "\n",
    "    # BW dataset\n",
    "    # info_path = f\"{base_dir}/BW_132_data/BW_stations_info.csv\"\n",
    "    # out_dir = f\"{base_dir}/BW_132_data\"\n",
    "\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_name = \"dist_angle_mat_test.npy\"\n",
    "    out_path = f\"{out_dir}/{out_name}\"\n",
    "    calc_dist_angle_mat(df_test, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_from_train_data(train_info_df, r_pos_mat):\n",
    "    # info_df = pd.read_csv(info_path)\n",
    "    # ori_r_pos_mat = np.load(relative_pos_mat_path)\n",
    "\n",
    "    # is_test = info_df[\"is_test\"].values\n",
    "    # train_mask = np.where(is_test == 0, True, False)\n",
    "    # train_info_df = info_df.loc[train_mask, :]\n",
    "\n",
    "    lat_mean, lat_std, lat_max, lat_min = train_info_df[\"lat\"].mean(), train_info_df[\"lat\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lat\"].max(), train_info_df[\"lat\"].min()\n",
    "    lon_mean, lon_std, lon_max, lon_min = train_info_df[\"lon\"].mean(), train_info_df[\"lon\"].std(ddof=0), \\\n",
    "                                          train_info_df[\"lon\"].max(), train_info_df[\"lon\"].min()\n",
    "\n",
    "    # indexes = np.where(train_mask)[0]\n",
    "    # idx_i, idx_j = np.ix_(indexes, indexes)\n",
    "    # r_pos_mat = ori_r_pos_mat[idx_i, idx_j, :]\n",
    "\n",
    "    r_dist_mat = r_pos_mat[:, :, 0]\n",
    "    r_angle_mat = r_pos_mat[:, :, 1]\n",
    "\n",
    "    r_dist_mean, r_dist_std, r_dist_max, r_dist_min = np.mean(r_dist_mat), np.std(r_dist_mat), \\\n",
    "                                                      np.max(r_dist_mat), np.min(r_dist_mat),\n",
    "    r_angle_mean, r_angle_std, r_angle_max, r_angle_min = np.mean(r_angle_mat), np.std(r_angle_mat), \\\n",
    "                                                          np.max(r_angle_mat), np.min(r_angle_mat)\n",
    "\n",
    "    stat_dict = {}\n",
    "    stat_dict[\"lat_mean\"], stat_dict[\"lat_std\"], stat_dict[\"lat_max\"], stat_dict[\"lat_min\"] = \\\n",
    "        lat_mean, lat_std, lat_max, lat_min\n",
    "    stat_dict[\"lon_mean\"], stat_dict[\"lon_std\"], stat_dict[\"lon_max\"], stat_dict[\"lon_min\"] = \\\n",
    "        lon_mean, lon_std, lon_max, lon_min\n",
    "\n",
    "    stat_dict[\"r_dist_mean\"], stat_dict[\"r_dist_std\"], stat_dict[\"r_dist_max\"], stat_dict[\"r_dist_min\"] = \\\n",
    "        r_dist_mean, r_dist_std, r_dist_max, r_dist_min\n",
    "    stat_dict[\"r_angle_mean\"], stat_dict[\"r_angle_std\"], stat_dict[\"r_angle_max\"], stat_dict[\"r_angle_min\"] = \\\n",
    "        r_angle_mean, r_angle_std, r_angle_max, r_angle_min\n",
    "\n",
    "    print(\"Calculates the statistics of training data. Done!\")\n",
    "\n",
    "    # with open(\"./data/hk_data_stats.pkl\".format(out_name), \"wb\") as fp:\n",
    "    #     pickle.dump(stat_dict, fp)\n",
    "\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_train.npy\")\n",
    "test_df=np.load(\"D:\\Spatial_interpolation\\data\\dist_angle_mat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculates the statistics of training data. Done!\n"
     ]
    }
   ],
   "source": [
    "stat_dict=generate_stat_from_train_data(df_train, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:, :, 0] = (train_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "train_df[:, :, 1] = (train_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[:, :, 0] = (test_df[:, :, 0] - stat_dict[\"r_dist_mean\"]) / stat_dict[\"r_dist_std\"]\n",
    "test_df[:, :, 1] = (test_df[:, :, 1] - stat_dict[\"r_angle_mean\"]) / stat_dict[\"r_angle_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test={\"test_data\":test_data,\"invalid_masks\":test_invid_mask,\"test_masks\":test_mask,\"r_pos_mat\":test_df,\"timestamps\":timestap_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train={\"train_data\":train_data,\"invalid_masks\":train_invid_mask,\"r_pos_mat\":train_df,\"timestamps\":timestaps_train_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105120,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"timestamps\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
