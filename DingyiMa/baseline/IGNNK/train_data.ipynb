{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Spatial-interpolation' in sys.path:\n",
    "    sys.path += ['Spatial-interpolation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Check the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import (load_nerl_data,\n",
    "    get_normalized_adj,\n",
    "    get_Laplace,\n",
    "    calculate_random_walk_matrix,\n",
    "    test_error_cap,\n",
    "    test_error\n",
    ")\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import math\n",
    "import pandas as pd\n",
    "from basic_structure import D_GCN, C_GCN, K_GCN,IGNNK\n",
    "from basic_process import *\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters and load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_o_n_m = 20 #sampled space dimension\n",
    "\n",
    "h = 24 #sampled time dimension\n",
    "\n",
    "z = 100 #hidden dimension for graph convolution\n",
    "\n",
    "K = 1 #If using diffusion convolution, the actual diffusion convolution step is K+1\n",
    "\n",
    "n_m = 20 #number of mask node during training\n",
    "\n",
    "N_u = 50 #target locations, N_u locations will be deleted from the training data\n",
    "\n",
    "Max_episode = 750 #max training episode\n",
    "\n",
    "learning_rate = 0.0001 #the learning_rate for Adam optimizer\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "E_maxvalue=80\n",
    "\n",
    "STmodel = IGNNK(h, z, K)\n",
    "STmodel.load_state_dict(torch.load('Spatial-interpolation/model/IGNNK_ushcn_750iter_2023-11-26 12:25:53.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: IGNNK-Dis-randsubgrh-3DGNN (Format:Metd-Adj-subgraph-Aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Import dataset from all txt files and csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathroot='/content/Spatial-interpolation/NDBC/all_stations'\n",
    "PATH_ROOT = os.getcwd()\n",
    "ROOT = os.path.join(PATH_ROOT, pathroot)\n",
    "filenames = os.listdir(ROOT)\n",
    "# Sort all files\n",
    "filenames.sort()\n",
    "data = []\n",
    "for i in filenames:\n",
    "    PATH_CSV = os.path.join(ROOT, i)\n",
    "    with open(PATH_CSV, 'r') as file:\n",
    "# Use splitlines() to divide contents in the documents into lists.\n",
    "        content_list = file.read().splitlines()\n",
    "# Transform lists into Numpy.\n",
    "    content_matrix = np.array([list(map(float, line.split())) for line in content_list])\n",
    "    data.append(content_matrix)\n",
    "data = np.array(data).transpose(1, 2, 0)\n",
    "X_raw=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: For loop on each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5,X_raw.shape[1]-1):\n",
    "    #Raw data and raw adjacency matrix without mapminmax\n",
    "    # X_raw=normalize_3d_array(X_raw)\n",
    "    Station_info=pd.read_csv('/content/Spatial-interpolation/NDBC/Station_info.csv')\n",
    "    NDBC_lat=pd.DataFrame(Station_info.iloc[:,1])\n",
    "    NDBC_long=pd.DataFrame(Station_info.iloc[:,3])\n",
    "    NDBC_ID=pd.DataFrame(Station_info.iloc[:,0])\n",
    "    Adj_dist=adj_dist(NDBC_lat,NDBC_long)\n",
    "    X_raw_0=X_raw[:,j,:]#GET the first feature\n",
    "    print(X_raw_0.shape)\n",
    "    # Count the num of missing values in each column\n",
    "    missing_counts_per_column = np.sum(np.isnan(X_raw_0), axis=0)\n",
    "    # print results\n",
    "    print(\"Incomplete data number in each column：\", missing_counts_per_column)\n",
    "\n",
    "    #Get the index if the value is not zero\n",
    "    # Find the columns where missing values exist.\n",
    "    columns_with_missing_data = np.any(np.isnan(X_raw_0), axis=0)\n",
    "\n",
    "    # Get the column numbers when missing value exist.\n",
    "    missing_columns = np.where(columns_with_missing_data)[0]\n",
    "\n",
    "    # Print results\n",
    "    print(\"The column numbers with missing values are：\", missing_columns)\n",
    "\n",
    "    #Get new data after deletion.\n",
    "    #Delete those columns(Stations) if there is not any features.\n",
    "    result = np.delete(X_raw_0, missing_columns, axis=1)\n",
    "    result = (result - np.min(result)) / (np.max(result) - np.min(result))\n",
    "    #Get new adjacency matrix after deletion.\n",
    "    NDBC_long=NDBC_long.transpose()\n",
    "    NDBC_long_aft=NDBC_long.drop(columns=missing_columns)\n",
    "    # Reconstruct the index in the DataFrame and get the longitude after deletion.\n",
    "    NDBC_long_aft = NDBC_long_aft.reset_index(drop=True)\n",
    "    print(NDBC_long_aft.shape)\n",
    "\n",
    "    NDBC_lat=NDBC_lat.transpose()\n",
    "    NDBC_lat_aft=NDBC_lat.drop(columns=missing_columns)\n",
    "    # Reconstruct the index in the DataFrame and get the latitude after deletion.\n",
    "    NDBC_lat_aft = NDBC_lat_aft.reset_index(drop=True)\n",
    "    print(NDBC_lat_aft.shape)\n",
    "    #GEt the new ADJ matrix.\n",
    "    Adj_dist=adj_dist(NDBC_lat_aft.transpose(),NDBC_long_aft.transpose())\n",
    "\n",
    "    capacities=np.ones((Adj_dist.shape[0], 1))\n",
    "    capacities=capacities.flatten()\n",
    "    split_line1 = int(result.shape[0] * 0.7)\n",
    "    training_set = result[:split_line1, :]\n",
    "    test_set = result[split_line1:, :]\n",
    "    rand = np.random.RandomState(0) # Fixed random output, just an example when seed = 0.\n",
    "    unknow_set = rand.choice(list(range(0,result.shape[1])),N_u,replace=True)#Stations to be predicted\n",
    "    unknow_set = set(unknow_set)\n",
    "    full_set = set(range(0,result.shape[1]))\n",
    "    know_set = full_set - unknow_set\n",
    "    training_set_s = training_set[:, list(know_set)]   # get the training data in the sample time period\n",
    "    A_s = Adj_dist[:, list(know_set)][list(know_set), :]\n",
    "    MAE_t, RMSE_t, R2_t, nrel_ignnk_res  = test_error_cap(STmodel, unknow_set, full_set,test_set, Adj_dist,h,capacities)\n",
    "    #MAE_t, RMSE_t, R2_t, nrel_ignnk_res  = test_error(STmodel, unknow_set, test_set, A_s,E_maxvalue, True)\n",
    "    print('Best model in the',j, '-th column is', MAE_t, RMSE_t, R2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
